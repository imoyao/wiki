---
title: RGW Bucket Shard ä¼˜åŒ–
categories: 
  - ğŸ’» å·¥ä½œ
  - å­˜å‚¨
  - CEPH
  - Ceph è¿ç»´
date: 2020-12-11 12:49:18
permalink: /pages/e2705a/
tags: 
  - 
---
# 1.bucket index èƒŒæ™¯ç®€ä»‹
bucket index æ˜¯æ•´ä¸ª RGW é‡Œé¢ä¸€ä¸ªéå¸¸å…³é”®çš„æ•°æ®ç»“æ„ï¼Œç”¨äºå­˜å‚¨ bucket çš„ç´¢å¼•æ•°æ®ï¼Œé»˜è®¤æƒ…å†µä¸‹å•ä¸ª bucket çš„ index å…¨éƒ¨å­˜å‚¨åœ¨ä¸€ä¸ª shard æ–‡ä»¶ï¼ˆshard æ•°é‡ä¸º 0ï¼Œä¸»è¦ä»¥ OMAP-keys æ–¹å¼å­˜å‚¨åœ¨ leveldb ä¸­ï¼‰ï¼Œéšç€å•ä¸ª bucket å†…çš„ Object æ•°é‡å¢åŠ ï¼Œæ•´ä¸ª shard æ–‡ä»¶çš„ä½“ç§¯ä¹Ÿåœ¨ä¸æ–­å¢é•¿ï¼Œå½“ shard æ–‡ä»¶ä½“ç§¯è¿‡å¤§å°±ä¼šå¼•å‘å„ç§é—®é¢˜ã€‚

# 2. é—®é¢˜åŠæ•…éšœ
## 2.1 æ•…éšœç°è±¡æè¿°
1. Flapping OSD's when RGW buckets have millions of objects
2. â— Possible causes
3. â—‹ The first issue here is when RGW buckets have millions of objects their
4. bucket index shard RADOS objects become very large with high
5. number OMAP keys stored in leveldb. Then operations like deep-scrub,
6. bucket index listing etc takes a lot of time to complete and this triggers
7. OSD's to flap. If sharding is not used this issue become worse because
8. then only one RADOS index objects will be holding all the OMAP keys.

RGW çš„ index æ•°æ®ä»¥ omap å½¢å¼å­˜å‚¨åœ¨ OSD æ‰€åœ¨èŠ‚ç‚¹çš„ leveldb ä¸­ï¼Œå½“å•ä¸ª bucket å­˜å‚¨çš„ Object æ•°é‡é«˜è¾¾ç™¾ä¸‡æ•°é‡çº§çš„æ—¶å€™ï¼Œ
deep-scrub å’Œ bucket list ä¸€ç±»çš„æ“ä½œå°†æå¤§çš„æ¶ˆè€—ç£ç›˜èµ„æºï¼Œå¯¼è‡´å¯¹åº” OSD å‡ºç°å¼‚å¸¸ï¼Œ
å¦‚æœä¸å¯¹ bucket çš„ index è¿›è¡Œ shard åˆ‡ç‰‡æ“ä½œ(shard åˆ‡ç‰‡å®ç°äº†å°†å•ä¸ª bucket index çš„ LevelDB å®ä¾‹æ°´å¹³åˆ‡åˆ†åˆ°å¤šä¸ª OSD ä¸Š)ï¼Œæ•°æ®é‡å¤§äº†ä»¥åå¾ˆå®¹æ˜“å‡ºäº‹ã€‚

1. â—‹ The second issue is when you have good amount of DELETEs it causes
2. loads of stale data in OMAP and this triggers leveldb compaction all the
3. time which is single threaded and non optimal with this kind of workload
4. and causes osd_op_threads to suicide because it is always compacting
5. hence OSDâ€™s starts flapping.

RGW åœ¨å¤„ç†å¤§é‡ DELETE è¯·æ±‚çš„æ—¶å€™ï¼Œä¼šå¯¼è‡´åº•å±‚ LevelDB é¢‘ç¹è¿›è¡Œæ•°æ®åº“ compaction(æ•°æ®å‹ç¼©ï¼Œå¯¹ç£ç›˜æ€§èƒ½æŸè€—å¾ˆå¤§)æ“ä½œï¼Œè€Œä¸”åˆšå¥½æ•´ä¸ª compaction åœ¨ LevelDB ä¸­åˆæ˜¯å•çº¿ç¨‹å¤„ç†ï¼Œå¾ˆå®¹æ˜“åˆ°è¾¾ osdopthreads è¶…æ—¶ä¸Šé™è€Œå¯¼è‡´ OSD è‡ªæ€ã€‚

**å¸¸è§çš„é—®é¢˜æœ‰:**
 1. å¯¹ index pool è¿›è¡Œ scrub æˆ– deep-scrub çš„æ—¶å€™ï¼Œå¦‚æœ shard å¯¹åº”çš„ Object è¿‡å¤§ï¼Œä¼šæå¤§æ¶ˆè€—åº•å±‚å­˜å‚¨è®¾å¤‡æ€§èƒ½ï¼Œé€ æˆ io è¯·æ±‚è¶…æ—¶ã€‚
 2. åº•å±‚ deep-scrub çš„æ—¶å€™è€—æ—¶è¿‡é•¿ï¼Œä¼šå‡ºç° request blockedï¼Œå¯¼è‡´å¤§é‡ http è¯·æ±‚è¶…æ—¶è€Œå‡ºç° 50x é”™è¯¯ï¼Œä»è€Œå½±å“åˆ°æ•´ä¸ª RGW æœåŠ¡çš„å¯ç”¨æ€§ã€‚
 3. å½“åç›˜æˆ–è€… osd æ•…éšœéœ€è¦æ¢å¤æ•°æ®çš„æ—¶å€™ï¼Œæ¢å¤ä¸€ä¸ªå¤§ä½“ç§¯çš„ shard æ–‡ä»¶å°†è€—å°½å­˜å‚¨èŠ‚ç‚¹æ€§èƒ½ï¼Œç”šè‡³å¯èƒ½å› ä¸º OSD å“åº”è¶…æ—¶è€Œå¯¼è‡´æ•´ä¸ªé›†ç¾¤å‡ºç°é›ªå´©ã€‚

## 2.2 æ ¹å› è·Ÿè¸ª
å½“ bucket index æ‰€åœ¨çš„ OSD omap è¿‡å¤§çš„æ—¶å€™ï¼Œä¸€æ—¦å‡ºç°å¼‚å¸¸å¯¼è‡´ OSD è¿›ç¨‹å´©æºƒï¼Œè¿™ä¸ªæ—¶å€™å°±éœ€è¦è¿›è¡Œç°åœº"æ•‘ç«"ï¼Œç”¨æœ€å¿«çš„é€Ÿåº¦æ¢å¤ OSD æœåŠ¡ã€‚
å…ˆç¡®å®šå¯¹åº” OSD çš„ OMAP å¤§å°ï¼Œè¿™ä¸ªè¿‡å¤§ä¼šå¯¼è‡´ OSD å¯åŠ¨çš„æ—¶å€™æ¶ˆè€—å¤§é‡æ—¶é—´å’Œèµ„æºå»åŠ è½½ levelDB æ•°æ®ï¼Œå¯¼è‡´ OSD æ— æ³•å¯åŠ¨ï¼ˆè¶…æ—¶è‡ªæ€ï¼‰ã€‚
ç‰¹åˆ«æ˜¯è¿™ä¸€ç±» OSD å¯åŠ¨éœ€è¦å ç”¨éå¸¸å¤§çš„å†…å­˜æ¶ˆè€—ï¼Œä¸€å®šè¦æ³¨æ„é¢„ç•™å¥½å†…å­˜ã€‚ï¼ˆç‰©ç†å†…å­˜ 40G å·¦å³ï¼Œä¸è¡Œç”¨ swap é¡¶ä¸Šï¼‰
![image.png](https://upload-images.jianshu.io/upload_images/2099201-1d0af9180a9fc1e9.png)

# 3. ä¸´æ—¶è§£å†³æ–¹æ¡ˆ
## 3.1 å…³é—­é›†ç¾¤ scrub, deep-scrub æå‡é›†ç¾¤ç¨³å®šæ€§
```shell
$ ceph osd set noscrub
$ ceph osd set nodeep-scrub
```
## 3.2 è°ƒé«˜ timeout å‚æ•°ï¼Œå‡å°‘ OSD è‡ªæ€çš„æ¦‚ç‡
```shell
osd_op_thread_timeout = 90 #default is 15
osd_op_thread_suicide_timeout = 2000 #default is 150
If filestore op threads are hitting timeout
filestore_op_thread_timeout = 180 #default is 60
filestore_op_thread_suicide_timeout = 2000 #default is 180
Same can be done for recovery thread also.
osd_recovery_thread_timeout = 120 #default is 30
osd_recovery_thread_suicide_timeout = 2000
```

## 3.2 æ‰‹å·¥å‹ç¼© OMAP
åœ¨å¯ä»¥åœ OSD çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥å¯¹ OSD è¿›è¡Œ compact æ“ä½œï¼Œæ¨èåœ¨ ceph 0.94.6 ä»¥ä¸Šç‰ˆæœ¬ï¼Œä½äºè¿™ä¸ªç‰ˆæœ¬æœ‰ bugã€‚Â [https://github.com/ceph/ceph/pull/7645/files](https://github.com/ceph/ceph/pull/7645/files)

 1. â—‹ The third temporary step could be taken if OSD's have very large OMAP
 2. directories you can verify it with command: du -sh /var/lib/ceph/osd/ceph-$id/current/omap, then do manual leveldb compaction for OSD's.
 3. â–  ceph tell osd.$id compact or
 4. â–  ceph daemon osd.$id compact or
 5. â–  Add leveldb_compact_on_mount = true in [osd.$id] or [osd] section
 6. and restart the OSD.
 7. â–  This makes sure that it compacts the leveldb and then bring the
 8. OSD back up/in which really helps.

```shell
#å¼€å¯nooutæ“ä½œ
$ ceph osd set noout

#åœOSDæœåŠ¡
$ systemctl stop ceph-osd@<osd-id>

#åœ¨ceph.confä¸­å¯¹åº”çš„[osd.id]åŠ ä¸Šä¸‹é¢é…ç½®
leveldb_compact_on_mount = true

#å¯åŠ¨osdæœåŠ¡
$ systemctl start ceph-osd@<osd-id>


#ä½¿ç”¨ceph -så‘½ä»¤è§‚å¯Ÿç»“æœï¼Œæœ€å¥½åŒæ—¶ä½¿ç”¨tailfå‘½ä»¤å»è§‚å¯Ÿå¯¹åº”çš„OSDæ—¥å¿—.ç­‰æ‰€æœ‰pgå¤„äºactive+cleanä¹‹åå†ç»§ç»­ä¸‹é¢çš„æ“ä½œ
$ ceph -s
#ç¡®è®¤compactå®Œæˆä»¥åçš„omapå¤§å°:
$ du -sh /var/lib/ceph/osd/ceph-$id/current/omap

#åˆ é™¤osdä¸­ä¸´æ—¶æ·»åŠ çš„leveldb_compact_on_mounté…ç½®

#å–æ¶ˆnooutæ“ä½œ(è§†æƒ…å†µè€Œå®šï¼Œå»ºè®®çº¿ä¸Šè¿˜æ˜¯ä¿ç•™noout):
$ ceph osd unset noout
```

# 4. æ°¸ä¹…è§£å†³æ–¹æ¡ˆ
## 4.1 æå‰è§„åˆ’å¥½ bucket shard
 - index pool ä¸€å®šè¦ä¸Š SSDï¼Œè¿™ä¸ªæ˜¯æœ¬æ–‡ä¼˜åŒ–çš„å‰æï¼Œæ²¡ç¡¬ä»¶æ”¯æ’‘åé¢è¿™äº›æ“ä½œéƒ½æ˜¯ç™½æ­ã€‚
 - åˆç†è®¾ç½® bucket çš„ shard æ•°é‡
shard çš„æ•°é‡å¹¶ä¸æ˜¯è¶Šå¤šè¶Šå¥½ï¼Œè¿‡å¤šçš„ shard ä¼šå¯¼è‡´éƒ¨åˆ†ç±»ä¼¼ list bucket çš„æ“ä½œæ¶ˆè€—å¤§é‡åº•å±‚å­˜å‚¨ IOï¼Œå¯¼è‡´éƒ¨åˆ†è¯·æ±‚è€—æ—¶è¿‡é•¿ã€‚
shard çš„æ•°é‡è¿˜è¦è€ƒè™‘åˆ°ä½  OSD çš„æ•…éšœéš”ç¦»åŸŸå’Œå‰¯æœ¬æ•°è®¾ç½®ã€‚æ¯”å¦‚ä½ è®¾ç½® index pool çš„ size ä¸º 2ï¼Œå¹¶ä¸”æœ‰ 2 ä¸ªæœºæŸœï¼Œå…± 24 ä¸ª OSD èŠ‚ç‚¹ï¼Œç†æƒ³æƒ…å†µä¸‹æ¯ä¸ª shard çš„ 2 ä¸ªå‰¯æœ¬éƒ½åº”è¯¥åˆ†å¸ƒåœ¨ 2 ä¸ªæœºæŸœé‡Œé¢ï¼Œæ¯”å¦‚å½“ä½  shard è®¾ç½®ä¸º 8 çš„æ—¶å€™ï¼Œæ€»å…±æœ‰ 8*2=16 ä¸ª shard æ–‡ä»¶éœ€è¦å­˜å‚¨ï¼Œé‚£ä¹ˆè¿™ 16 ä¸ª shard è¦åšåˆ°å‡åˆ†åˆ° 2 ä¸ªæœºæŸœã€‚åŒæ—¶å¦‚æœä½  shard è¶…è¿‡ 24 ä¸ªï¼Œè¿™å¾ˆæ˜æ˜¾ä¹Ÿæ˜¯ä¸åˆé€‚çš„ã€‚
 - æ§åˆ¶å¥½å•ä¸ª bucket index shard çš„å¹³å‡ä½“ç§¯ï¼Œç›®å‰æ¨èå•ä¸ª shard å­˜å‚¨çš„ Object ä¿¡æ¯æ¡ç›®åœ¨ 10-15W å·¦å³ï¼Œè¿‡å¤šåˆ™éœ€è¦å¯¹ç›¸åº”çš„ bucket åšå•ç‹¬ reshard æ“ä½œï¼ˆæ³¨æ„è¿™ä¸ªæ˜¯é«˜å±æ“ä½œï¼Œè°¨æ…ä½¿ç”¨ï¼‰ã€‚æ¯”å¦‚ä½ é¢„è®¡å•ä¸ª bucket æœ€å¤šå­˜å‚¨ 100W ä¸ª Objectï¼Œé‚£ä¹ˆ 100W/8ï¼12.5Wï¼Œè®¾ç½® shard æ•°ä¸º 8 æ˜¯æ¯”è¾ƒåˆç†çš„ã€‚shard æ–‡ä»¶ä¸­æ¯æ¡ omapkey è®°å½•å¤§æ¦‚å ç”¨ 200 byte çš„å®¹é‡ï¼Œé‚£ä¹ˆ 150000*200/1024/1024 â‰ˆ 28.61 MBï¼Œä¹Ÿå°±æ˜¯è¯´è¦æ§åˆ¶å•ä¸ª shard æ–‡ä»¶çš„ä½“ç§¯åœ¨ 28MB ä»¥å†…ã€‚

 - ä¸šåŠ¡å±‚é¢æ§åˆ¶å¥½æ¯ä¸ª bucket çš„ Object ä¸Šé™ï¼ŒæŒ‰æ¯ä¸ª shard æ–‡ä»¶å¹³å‡ 10-15W Object ä¸ºå®œã€‚

### 4.1.1 é…ç½® Bucket Index Sharding

To enable and configure bucket index sharding on all new buckets, use:Â Â [redhat-bucket_sharding](https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/1.3/html/object_gateway_guide_for_red_hat_enterprise_linux/administration_cli#bucket_sharding)
*   theÂ `rgw_override_bucket_index_max_shards`Â setting for simple configurations,
*   theÂ `bucket_index_max_shards`Â setting for federated configurations

**Simple configurationsï¼š**
```plain
#1. ä¿®æ”¹é…ç½®æ–‡ä»¶è®¾ç½®ç›¸åº”çš„å‚æ•°ã€‚ Note that maximum number of shards is 7877.
[global]
rgw_override_bucket_index_max_shards = 10
#2. é‡å¯rgwæœåŠ¡ï¼Œè®©å…¶ç”Ÿæ•ˆ
systemctl restart ceph-radosgw.target

#3. æŸ¥çœ‹bucket shardæ•°
rados -p default.rgw.buckets.index ls | wc -l
1000
```
**Federated configurations**
In federated configurations, each zone can have a different index_pool setting to manage failover. To configure a consistent shard count for zones in one region, set the bucket_index_max_shards setting in the configuration for that region. To do so:

```plain
#1. Extract the region configuration to the region.json file:
$ radosgw-admin region get > region.json

#2. In the region.json file, set the bucket_index_max_shards setting for each named zone.

#3. Reset the region:
$ radosgw-admin region set < region.json

#4. Update the region map:
$ radosgw-admin regionmap update --name <name>

#5. Replace <name> with the name of the Ceph Object Gateway user, for example:
$ radosgw-admin regionmap update --name client.rgw.ceph-client
```

**ä¸Šä¼ æ–‡ä»¶ Demo:**
```python
#_*_coding:utf-8_*_
#yum install python-boto
import boto
import boto.s3.connection
#pip install filechunkio
from filechunkio import  FileChunkIO
import math
import  threading
import os
import Queue
class Chunk(object):
    num = 0
    offset = 0
    len = 0
    def __init__(self,n,o,l):
        self.num=n
        self.offset=o
        self.length=l
class CONNECTION(object):
    def __init__(self,access_key,secret_key,ip,port,is_secure=False,chrunksize=8<<20): #chunksizeæœ€å°8Må¦åˆ™ä¸Šä¼ è¿‡ç¨‹ä¼šæŠ¥é”™
        self.conn=boto.connect_s3(
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        host=ip,port=port,
        is_secure=is_secure,
        calling_format=boto.s3.connection.OrdinaryCallingFormat()
        )
        self.chrunksize=chrunksize
        self.port=port
    #æŸ¥è¯¢
    def list_all(self):
        all_buckets=self.conn.get_all_buckets()
        for bucket in all_buckets:
            print u'å®¹å™¨å: %s' %(bucket.name)
            for key in bucket.list():
                print ' '*5,"%-20s%-20s%-20s%-40s%-20s" %(key.mode,key.owner.id,key.size,key.last_modified.split('.')[0],key.name)
    def list_single(self,bucket_name):
        try:
            single_bucket = self.conn.get_bucket(bucket_name)
        except Exception as e:
            print 'bucket %s is not exist' %bucket_name
            return
        print u'å®¹å™¨å: %s' % (single_bucket.name)
        for key in single_bucket.list():
            print ' ' * 5, "%-20s%-20s%-20s%-40s%-20s" % (key.mode, key.owner.id, key.size, key.last_modified.split('.')[0], key.name)
    #æ™®é€šå°æ–‡ä»¶ä¸‹è½½ï¼šæ–‡ä»¶å¤§å°<=8M
    def dowload_file(self,filepath,key_name,bucket_name):
        all_bucket_name_list = [i.name for i in self.conn.get_all_buckets()]
        if bucket_name not in all_bucket_name_list:
            print 'Bucket %s is not exist,please try again' % (bucket_name)
            return
        else:
            bucket = self.conn.get_bucket(bucket_name)
        all_key_name_list = [i.name for i in bucket.get_all_keys()]
        if key_name not in all_key_name_list:
            print 'File %s is not exist,please try again' % (key_name)
            return
        else:
            key = bucket.get_key(key_name)
        if not os.path.exists(os.path.dirname(filepath)):
            print 'Filepath %s is not exists, sure to create and try again' % (filepath)
            return
        if os.path.exists(filepath):
            while True:
                d_tag = raw_input('File %s already exists, sure you want to cover (Y/N)?' % (key_name)).strip()
                if d_tag not in ['Y', 'N'] or len(d_tag) == 0:
                    continue
                elif d_tag == 'Y':
                    os.remove(filepath)
                    break
                elif d_tag == 'N':
                    return
        os.mknod(filepath)
        try:
            key.get_contents_to_filename(filepath)
        except Exception:
            pass
    # æ™®é€šå°æ–‡ä»¶ä¸Šä¼ ï¼šæ–‡ä»¶å¤§å°<=8M
    def upload_file(self,filepath,key_name,bucket_name):
        try:
            bucket = self.conn.get_bucket(bucket_name)
        except Exception as e:
            print 'bucket %s is not exist' % bucket_name
            tag = raw_input('Do you want to create the bucket %s: (Y/N)?' % bucket_name).strip()
            while tag not in ['Y', 'N']:
                tag = raw_input('Please input (Y/N)').strip()
            if tag == 'N':
                return
            elif tag == 'Y':
                self.conn.create_bucket(bucket_name)
                bucket = self.conn.get_bucket(bucket_name)
        all_key_name_list = [i.name for i in bucket.get_all_keys()]
        if key_name in all_key_name_list:
            while True:
                f_tag = raw_input(u'File already exists, sure you want to cover (Y/N)?: ').strip()
                if f_tag not in ['Y', 'N'] or len(f_tag) == 0:
                    continue
                elif f_tag == 'Y':
                    break
                elif f_tag == 'N':
                    return
        key=bucket.new_key(key_name)
        if not os.path.exists(filepath):
            print 'File %s does not exist, please make sure you want to upload file path and try again' %(key_name)
            return
        try:
            f=file(filepath,'rb')
            data=f.read()
            key.set_contents_from_string(data)
        except Exception:
            pass
    def delete_file(self,key_name,bucket_name):
        all_bucket_name_list = [i.name for i in self.conn.get_all_buckets()]
        if bucket_name not in all_bucket_name_list:
            print 'Bucket %s is not exist,please try again' % (bucket_name)
            return
        else:
            bucket = self.conn.get_bucket(bucket_name)
        all_key_name_list = [i.name for i in bucket.get_all_keys()]
        if key_name not in all_key_name_list:
            print 'File %s is not exist,please try again' % (key_name)
            return
        else:
            key = bucket.get_key(key_name)
        try:
            bucket.delete_key(key.name)
        except Exception:
            pass
    def delete_bucket(self,bucket_name):
        all_bucket_name_list = [i.name for i in self.conn.get_all_buckets()]
        if bucket_name not in all_bucket_name_list:
            print 'Bucket %s is not exist,please try again' % (bucket_name)
            return
        else:
            bucket = self.conn.get_bucket(bucket_name)
        try:
            self.conn.delete_bucket(bucket.name)
        except Exception:
            pass

    #é˜Ÿåˆ—ç”Ÿæˆ
    def init_queue(self,filesize,chunksize):   #8<<20 :8*2**20
        chunkcnt=int(math.ceil(filesize*1.0/chunksize))
        q=Queue.Queue(maxsize=chunkcnt)
        for i in range(0,chunkcnt):
            offset=chunksize*i
            length=min(chunksize,filesize-offset)
            c=Chunk(i+1,offset,length)
            q.put(c)
        return q
    #åˆ†ç‰‡ä¸Šä¼ object
    def upload_trunk(self,filepath,mp,q,id):
        while not q.empty():
            chunk=q.get()
            fp=FileChunkIO(filepath,'r',offset=chunk.offset,bytes=chunk.length)
            mp.upload_part_from_file(fp,part_num=chunk.num)
            fp.close()
            q.task_done()
    #æ–‡ä»¶å¤§å°è·å–---->S3åˆ†ç‰‡ä¸Šä¼ å¯¹è±¡ç”Ÿæˆ----->åˆå§‹é˜Ÿåˆ—ç”Ÿæˆ(--------------->æ–‡ä»¶åˆ‡ï¼Œç”Ÿæˆåˆ‡åˆ†å¯¹è±¡)
    def upload_file_multipart(self,filepath,key_name,bucket_name,threadcnt=8):
        filesize=os.stat(filepath).st_size
        try:
            bucket=self.conn.get_bucket(bucket_name)
        except Exception as e:
            print 'bucket %s is not exist' % bucket_name
            tag=raw_input('Do you want to create the bucket %s: (Y/N)?' %bucket_name).strip()
            while tag not in ['Y','N']:
                tag=raw_input('Please input (Y/N)').strip()
            if tag == 'N':
                return
            elif tag == 'Y':
                self.conn.create_bucket(bucket_name)
                bucket = self.conn.get_bucket(bucket_name)
        all_key_name_list=[i.name for i in bucket.get_all_keys()]
        if key_name  in all_key_name_list:
            while True:
                f_tag=raw_input(u'File already exists, sure you want to cover (Y/N)?: ').strip()
                if f_tag not in ['Y','N'] or len(f_tag) == 0:
                    continue
                elif f_tag == 'Y':
                    break
                elif f_tag == 'N':
                    return
        mp=bucket.initiate_multipart_upload(key_name)
        q=self.init_queue(filesize,self.chrunksize)
        for i in range(0,threadcnt):
            t=threading.Thread(target=self.upload_trunk,args=(filepath,mp,q,i))
            t.setDaemon(True)
            t.start()
        q.join()
        mp.complete_upload()
    #æ–‡ä»¶åˆ†ç‰‡ä¸‹è½½
    def download_chrunk(self,filepath,key_name,bucket_name,q,id):
        while not q.empty():
            chrunk=q.get()
            offset=chrunk.offset
            length=chrunk.length
            bucket=self.conn.get_bucket(bucket_name)
            resp=bucket.connection.make_request('GET',bucket_name,key_name,headers={'Range':"bytes=%d-%d" %(offset,offset+length)})
            data=resp.read(length)
            fp=FileChunkIO(filepath,'r+',offset=chrunk.offset,bytes=chrunk.length)
            fp.write(data)
            fp.close()
            q.task_done()
    def download_file_multipart(self,filepath,key_name,bucket_name,threadcnt=8):
        all_bucket_name_list=[i.name for i in self.conn.get_all_buckets()]
        if bucket_name not in all_bucket_name_list:
            print 'Bucket %s is not exist,please try again' %(bucket_name)
            return
        else:
            bucket=self.conn.get_bucket(bucket_name)
        all_key_name_list = [i.name for i in bucket.get_all_keys()]
        if key_name not in all_key_name_list:
            print 'File %s is not exist,please try again' %(key_name)
            return
        else:
            key=bucket.get_key(key_name)
        if not os.path.exists(os.path.dirname(filepath)):
            print 'Filepath %s is not exists, sure to create and try again' % (filepath)
            return
        if os.path.exists(filepath):
            while True:
                d_tag = raw_input('File %s already exists, sure you want to cover (Y/N)?' % (key_name)).strip()
                if d_tag not in ['Y', 'N'] or len(d_tag) == 0:
                    continue
                elif d_tag == 'Y':
                    os.remove(filepath)
                    break
                elif d_tag == 'N':
                    return
        os.mknod(filepath)
        filesize=key.size
        q=self.init_queue(filesize,self.chrunksize)
        for i in range(0,threadcnt):
            t=threading.Thread(target=self.download_chrunk,args=(filepath,key_name,bucket_name,q,i))
            t.setDaemon(True)
            t.start()
        q.join()
    def generate_object_download_urls(self,key_name,bucket_name,valid_time=0):
        all_bucket_name_list = [i.name for i in self.conn.get_all_buckets()]
        if bucket_name not in all_bucket_name_list:
            print 'Bucket %s is not exist,please try again' % (bucket_name)
            return
        else:
            bucket = self.conn.get_bucket(bucket_name)
        all_key_name_list = [i.name for i in bucket.get_all_keys()]
        if key_name not in all_key_name_list:
            print 'File %s is not exist,please try again' % (key_name)
            return
        else:
            key = bucket.get_key(key_name)
        try:
            key.set_canned_acl('public-read')
            download_url = key.generate_url(valid_time, query_auth=False, force_http=True)
            if self.port != 80:
                x1=download_url.split('/')[0:3]
                x2=download_url.split('/')[3:]
                s1=u'/'.join(x1)
                s2=u'/'.join(x2)
                s3=':%s/' %(str(self.port))
                download_url=s1+s3+s2
                print download_url
        except Exception:
            pass
if __name__ == '__main__':
    #çº¦å®šï¼š
    #1:filepathæŒ‡æœ¬åœ°æ–‡ä»¶çš„è·¯å¾„(ä¸Šä¼ è·¯å¾„orä¸‹è½½è·¯å¾„),æŒ‡çš„æ˜¯ç»å¯¹è·¯å¾„
    #2:bucket_nameç›¸å½“äºæ–‡ä»¶åœ¨å¯¹è±¡å­˜å‚¨ä¸­çš„ç›®å½•åæˆ–è€…ç´¢å¼•å
    #3:key_nameç›¸å½“äºæ–‡ä»¶åœ¨å¯¹è±¡å­˜å‚¨ä¸­å¯¹åº”çš„æ–‡ä»¶åæˆ–æ–‡ä»¶ç´¢å¼•
    access_key = "FYT71CYU3UQKVMC8YYVY"
    secret_key = "rVEASbWAytjVLv1G8Ta8060lY3yrcdPTsEL0rfwr"
    ip='127.0.0.1'
    port=7480
    conn=CONNECTION(access_key,secret_key,ip,port)
    #æŸ¥çœ‹æ‰€æœ‰bucketä»¥åŠå…¶åŒ…å«çš„æ–‡ä»¶
    #conn.list_all()
    #ç®€å•ä¸Šä¼ ,ç”¨äºæ–‡ä»¶å¤§å°<=8M
    #conn.upload_file('/etc/passwd','passwd','test_bucket01')
    conn.upload_file('/tmp/test.log','test1','test_bucket12')
    #æŸ¥çœ‹å•ä¸€bucketä¸‹æ‰€åŒ…å«çš„æ–‡ä»¶ä¿¡æ¯
    conn.list_single('test_bucket12')

    #ç®€å•ä¸‹è½½,ç”¨äºæ–‡ä»¶å¤§å°<=8M
    # conn.dowload_file('/lhf_test/test01','passwd','test_bucket01')
    # conn.list_single('test_bucket01')
    #åˆ é™¤æ–‡ä»¶
    # conn.delete_file('passwd','test_bucket01')
    # conn.list_single('test_bucket01')
    #
    #åˆ é™¤bucket
    # conn.delete_bucket('test_bucket01')
    # conn.list_all()
    #åˆ‡ç‰‡ä¸Šä¼ (å¤šçº¿ç¨‹),ç”¨äºæ–‡ä»¶å¤§å°>8M,8Må¯ä¿®æ”¹ï¼Œä½†ä¸èƒ½å°äº8M,å¦åˆ™ä¼šæŠ¥é”™åˆ‡ç‰‡å¤ªå°
    # conn.upload_file_multipart('/etc/passwd','passwd_multi_upload','test_bucket01')
    # conn.list_single('test_bucket01')
    # åˆ‡ç‰‡ä¸‹è½½(å¤šçº¿ç¨‹),ç”¨äºæ–‡ä»¶å¤§å°>8M,8Må¯ä¿®æ”¹ï¼Œä½†ä¸èƒ½å°äº8Mï¼Œå¦åˆ™ä¼šæŠ¥é”™åˆ‡ç‰‡å¤ªå°
    # conn.download_file_multipart('/lhf_test/passwd_multi_dowload','passwd_multi_upload','test_bucket01')
    #ç”Ÿæˆä¸‹è½½url
    #conn.generate_object_download_urls('passwd_multi_upload','test_bucket01')
    #conn.list_all()
```

## 4.2 å¯¹ bucket åš reshard æ“ä½œ

To reshard the bucket index pool:Â [redhat-bucket_sharding](https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/1.3/html/object_gateway_guide_for_red_hat_enterprise_linux/administration_cli#bucket_sharding)

```plain
#æ³¨æ„ä¸‹é¢çš„æ“ä½œä¸€å®šè¦ç¡®ä¿å¯¹åº”çš„bucketç›¸å…³çš„æ“ä½œéƒ½å·²ç»å…¨éƒ¨åœæ­¢ï¼Œä¹‹åä½¿ç”¨ä¸‹é¢å‘½ä»¤å¤‡ä»½bucketçš„index
$ radosgw-admin bi list --bucket=<bucket_name> > <bucket_name>.list.backup

#é€šè¿‡ä¸‹é¢çš„å‘½ä»¤æ¢å¤æ•°æ®
$ radosgw-admin bi put --bucket=<bucket_name> < <bucket_name>.list.backup

#æŸ¥çœ‹bucketçš„index id
$ radosgw-admin bucket stats --bucket=bucket-maillist
{
    "bucket": "bucket-maillist",
    "pool": "default.rgw.buckets.data",
    "index_pool": "default.rgw.buckets.index",
    "id": "0a6967a5-2c76-427a-99c6-8a788ca25034.54133.1", #æ³¨æ„è¿™ä¸ªid
    "marker": "0a6967a5-2c76-427a-99c6-8a788ca25034.54133.1",
    "owner": "user",
    "ver": "0#1,1#1",
    "master_ver": "0#0,1#0",
    "mtime": "2017-08-23 13:42:59.007081",
    "max_marker": "0#,1#",
    "usage": {},
    "bucket_quota": {
        "enabled": false,
        "max_size_kb": -1,
        "max_objects": -1
    }
}


#Reshardå¯¹åº”bucketçš„indexæ“ä½œå¦‚ä¸‹:
#ä½¿ç”¨å‘½ä»¤å°†"bucket-maillist"çš„shardè°ƒæ•´ä¸º4ï¼Œæ³¨æ„å‘½ä»¤ä¼šè¾“å‡ºosdå’Œnewä¸¤ä¸ªbucketçš„instance id

$ radosgw-admin bucket reshard --bucket="bucket-maillist" --num-shards=4
*** NOTICE: operation will not remove old bucket index objects ***
***         these will need to be removed manually             ***
old bucket instance id: 0a6967a5-2c76-427a-99c6-8a788ca25034.54133.1
new bucket instance id: 0a6967a5-2c76-427a-99c6-8a788ca25034.54147.1
total entries: 3


#ä¹‹åä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤åˆ é™¤æ—§çš„instance id

$ radosgw-admin bi purge --bucket="bucket-maillist" --bucket-id=0a6967a5-2c76-427a-99c6-8a788ca25034.54133.1

#æŸ¥çœ‹æœ€ç»ˆç»“æœ
$ radosgw-admin bucket stats --bucket=bucket-maillist
{
    "bucket": "bucket-maillist",
    "pool": "default.rgw.buckets.data",
    "index_pool": "default.rgw.buckets.index",
    "id": "0a6967a5-2c76-427a-99c6-8a788ca25034.54147.1", #idå·²ç»å˜æ›´
    "marker": "0a6967a5-2c76-427a-99c6-8a788ca25034.54133.1",
    "owner": "user",
    "ver": "0#2,1#1,2#1,3#2",
    "master_ver": "0#0,1#0,2#0,3#0",
    "mtime": "2017-08-23 14:02:19.961205",
    "max_marker": "0#,1#,2#,3#",
    "usage": {
        "rgw.main": {
            "size_kb": 50,
            "size_kb_actual": 60,
            "num_objects": 3
        }
    },
    "bucket_quota": {
        "enabled": false,
        "max_size_kb": -1,
        "max_objects": -1
    }
}
```
