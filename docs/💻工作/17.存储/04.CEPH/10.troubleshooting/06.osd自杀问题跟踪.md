---
title: osd è‡ªæ€é—®é¢˜è·Ÿè¸ª

categories: 
  - ğŸ’»å·¥ä½œ
  - å­˜å‚¨
  - CEPH
  - troubleshooting
date: 2020-05-23 11:02:28
tags: null
permalink: /pages/553cd7/
---

## 1. è¯´æ˜
 **æ•…éšœç°è±¡æè¿°ï¼š**
```plain
Flapping OSD's when RGW buckets have millions of objects
â— Possible causes
â—‹ The first issue here is when RGW buckets have millions of objects their
bucket index shard RADOS objects become very large with high
number OMAP keys stored in leveldb. Then operations like deep-scrub,
bucket index listing etc takes a lot of time to complete and this triggers
OSD's to flap. If sharding is not used this issue become worse because
then only one RADOS index objects will be holding all the OMAP keys.
```
RGW çš„ index æ•°æ®ä»¥ omap å½¢å¼å­˜å‚¨åœ¨ OSD æ‰€åœ¨èŠ‚ç‚¹çš„ leveldb ä¸­ï¼Œå½“å•ä¸ª bucket å­˜å‚¨çš„ Object æ•°é‡é«˜è¾¾ç™¾ä¸‡æ•°é‡çº§çš„æ—¶å€™ï¼Œdeep-scrub å’Œ bucket list ä¸€ç±»çš„æ“ä½œå°†æå¤§çš„æ¶ˆè€—ç£ç›˜èµ„æºï¼Œå¯¼è‡´å¯¹åº” OSD å‡ºç°å¼‚å¸¸ï¼Œå¦‚æœä¸å¯¹ bucket çš„ index è¿›è¡Œ shard åˆ‡ç‰‡æ“ä½œ(shard åˆ‡ç‰‡å®ç°äº†å°†å•ä¸ª bucket index çš„ LevelDB å®ä¾‹æ°´å¹³åˆ‡åˆ†åˆ°å¤šä¸ª OSD ä¸Š)ï¼Œæ•°æ®é‡å¤§äº†ä»¥åå¾ˆå®¹æ˜“å‡ºäº‹ã€‚

```plain
â—‹ The second issue is when you have good amount of DELETEs it causes
loads of stale data in OMAP and this triggers leveldb compaction all the
time which is single threaded and non optimal with this kind of workload
and causes osd_op_threads to suicide because it is always compacting
hence OSDâ€™s starts flapping.
```
RGW åœ¨å¤„ç†å¤§é‡ DELETE è¯·æ±‚çš„æ—¶å€™ï¼Œä¼šå¯¼è‡´åº•å±‚ LevelDB é¢‘ç¹è¿›è¡Œæ•°æ®åº“ compaction(æ•°æ®å‹ç¼©ï¼Œå¯¹ç£ç›˜æ€§èƒ½æŸè€—å¾ˆå¤§)æ“ä½œï¼Œè€Œä¸”åˆšå¥½æ•´ä¸ª compaction åœ¨ LevelDB ä¸­åˆæ˜¯å•çº¿ç¨‹å¤„ç†ï¼Œå¾ˆå®¹æ˜“åˆ°è¾¾ osdopthreads è¶…æ—¶ä¸Šé™è€Œå¯¼è‡´ OSD è‡ªæ€ã€‚

```plain
â— Possible causes contd ...
â—‹ OMAP backend is leveldb in jewel and older clusters. Any luminous
clusters which were upgraded from older releases have leveldb as
OMAP backend.
```
jewel ä»¥åŠä¹‹å‰çš„ç‰ˆæœ¬çš„ OMAP éƒ½æ˜¯ä»¥ LevelDB ä½œä¸ºå­˜å‚¨å¼•æ“ï¼Œå¦‚æœæ˜¯ä»æ—§ç‰ˆæœ¬å‡çº§åˆ°æœ€æ–°çš„ luminousï¼Œé‚£ä¹ˆåº•å±‚ OMAP ä»ç„¶æ˜¯ LevelDBã€‚
```plain
â—‹ All new luminous clusters have default OMAP backend as rocksdb
which is great because rocksdb has multithreaded compaction and in
Ceph we use 8 compaction thread by default and many other enhanced
features as compare to leveldb.
```
æœ€æ–°ç‰ˆæœ¬çš„ Luminous å¼€å§‹ï¼ŒOMAP åº•å±‚çš„å­˜å‚¨å¼•æ“æ¢æˆäº† rocksDBï¼ŒrocksDB é‡‡ç”¨å¤šçº¿ç¨‹æ–¹å¼è¿›è¡Œ compaction(é»˜è®¤ 8 ä¸ªï¼‰ï¼Œæ‰€ä»¥ rocksdb åœ¨ compaction æ•ˆç‡ä¸Šè¦æ¯” LevelDB å¼ºå¾ˆå¤šã€‚

## 2. æ ¹å› è·Ÿè¸ª
å½“ bucket index æ‰€åœ¨çš„ OSD omap è¿‡å¤§çš„æ—¶å€™ï¼Œä¸€æ—¦å‡ºç°å¼‚å¸¸å¯¼è‡´ OSD è¿›ç¨‹å´©æºƒï¼Œè¿™ä¸ªæ—¶å€™å°±éœ€è¦è¿›è¡Œç°åœº"æ•‘ç«"ï¼Œç”¨æœ€å¿«çš„é€Ÿåº¦æ¢å¤ OSD æœåŠ¡ã€‚

å…ˆç¡®å®šå¯¹åº” OSD çš„ OMAP å¤§å°ï¼Œè¿™ä¸ªè¿‡å¤§ä¼šå¯¼è‡´ OSD å¯åŠ¨çš„æ—¶å€™æ¶ˆè€—å¤§é‡æ—¶é—´å’Œèµ„æºå»åŠ è½½ levelDB æ•°æ®ï¼Œå¯¼è‡´ OSD æ— æ³•å¯åŠ¨ï¼ˆè¶…æ—¶è‡ªæ€ï¼‰ã€‚

ç‰¹åˆ«æ˜¯è¿™ä¸€ç±» OSD å¯åŠ¨éœ€è¦å ç”¨éå¸¸å¤§çš„å†…å­˜æ¶ˆè€—ï¼Œä¸€å®šè¦æ³¨æ„é¢„ç•™å¥½å†…å­˜ã€‚ï¼ˆç‰©ç†å†…å­˜ 40G å·¦å³ï¼Œä¸è¡Œç”¨ swap é¡¶ä¸Šï¼‰
![image.png](https://upload-images.jianshu.io/upload_images/2099201-48d1c1c182999c28.png)

## 3. ä¿®å¤æ–¹å¼
### 3.1 ä¸´æ—¶è§£å†³æ–¹æ¡ˆ
1. å…³é—­é›†ç¾¤ scrub, deep-scrub æå‡é›†ç¾¤ç¨³å®šæ€§
```plain
$ ceph osd set noscrub
$ ceph osd set nodeep-scrub
```
2. è°ƒé«˜ timeout å‚æ•°ï¼Œå‡å°‘ OSD è‡ªæ€çš„æ¦‚ç‡
```plain
osd_op_thread_timeout = 90 #default is 15
osd_op_thread_suicide_timeout = 2000 #default is 150
If filestore op threads are hitting timeout
filestore_op_thread_timeout = 180 #default is 60
filestore_op_thread_suicide_timeout = 2000 #default is 180
Same can be done for recovery thread also.
osd_recovery_thread_timeout = 120 #default is 30
osd_recovery_thread_suicide_timeout = 2000 #default is 300
```

3. æ‰‹å·¥å‹ç¼© LevelDB OMAP
åœ¨å¯ä»¥åœ OSD çš„æƒ…å†µä¸‹ï¼Œå¯ä»¥å¯¹ OSD è¿›è¡Œ compact æ“ä½œï¼Œæ¨èåœ¨ ceph 0.94.6 ä»¥ä¸Šç‰ˆæœ¬ï¼Œä½äºè¿™ä¸ªç‰ˆæœ¬æœ‰ bugã€‚Â [https://github.com/ceph/ceph/pull/7645/files](https://github.com/ceph/ceph/pull/7645/files)
```plain
â—‹ The third temporary step could be taken if OSD's have very large OMAP
directories you can verify it with command: du -sh /var/lib/ceph/osd/ceph-$id/current/omap, then do manual leveldb compaction for OSD's.
â–  ceph tell osd.$id compact or
â–  ceph daemon osd.$id compact or
â–  Add leveldb_compact_on_mount = true in [osd.$id] or [osd] section
and restart the OSD.
â–  This makes sure that it compacts the leveldb and then bring the
OSD back up/in which really helps.
```
```plain
#å¼€å¯nooutæ“ä½œ
$ ceph osd set noout

#åœOSDæœåŠ¡
$ systemctl stop ceph-osd@<osd-id>

#åœ¨ceph.confä¸­å¯¹åº”çš„[osd.id]åŠ ä¸Šä¸‹é¢é…ç½®
leveldb_compact_on_mount = true

#å¯åŠ¨osdæœåŠ¡
$ systemctl start ceph-osd@<osd-id>


#ä½¿ç”¨ceph -så‘½ä»¤è§‚å¯Ÿç»“æœï¼Œæœ€å¥½åŒæ—¶ä½¿ç”¨tailfå‘½ä»¤å»è§‚å¯Ÿå¯¹åº”çš„OSDæ—¥å¿—.ç­‰æ‰€æœ‰pgå¤„äºactive+cleanä¹‹åå†ç»§ç»­ä¸‹é¢çš„æ“ä½œ
$ ceph -s
#ç¡®è®¤compactå®Œæˆä»¥åçš„omapå¤§å°:
du -sh /var/lib/ceph/osd/ceph-$id/current/omap

#åˆ é™¤osdä¸­ä¸´æ—¶æ·»åŠ çš„leveldb_compact_on_mounté…ç½®

#å–æ¶ˆnooutæ“ä½œ(è§†æƒ…å†µè€Œå®šï¼Œå»ºè®®çº¿ä¸Šè¿˜æ˜¯ä¿ç•™noout):
ceph osd unset noout
```

### 3.2 æ°¸ä¹…æ–¹æ¡ˆ
#### 3.2.1 å¯¹ bucket åš reshard æ“ä½œ
å¯¹ bucket åš reshard æ“ä½œï¼Œå¯ä»¥å®ç°è°ƒæ•´ bucket çš„ shard æ•°é‡ï¼Œå®ç° index æ•°æ®çš„é‡æ–°åˆ†å¸ƒã€‚ ä»…æ”¯æŒ ceph 0.94.10 ä»¥ä¸Šç‰ˆæœ¬ï¼Œéœ€è¦åœ bucket è¯»å†™ï¼Œæœ‰æ•°æ®ä¸¢å¤±é£é™©ï¼Œæ…é‡ä½¿ç”¨ã€‚

å¦å¤–æœ€æ–°çš„ Luminous å¯ä»¥å®ç°åŠ¨æ€çš„ reshard(æ ¹æ®å•ä¸ª bucket å½“å‰çš„ Object æ•°é‡ï¼Œå®æ—¶åŠ¨æ€è°ƒæ•´ shard æ•°é‡ï¼‰ï¼Œå…¶å®è¿™é‡Œé¢ä¹Ÿæœ‰å¾ˆå¤§çš„å‘ï¼ŒåŠ¨æ€ reshard å¯¹ç”¨æˆ·æ¥è®²ä¸å¤Ÿé€æ˜ï¼Œè€Œä¸” reshard è¿‡ç¨‹ä¸­ä¼šé€ æˆ bucket çš„è¯»å†™å‘ç”Ÿä¸€å®šæ—¶é—´çš„é˜»å¡ï¼Œæ‰€ä»¥ä»æˆ‘çš„ä¸ªäººç»éªŒæ¥çœ‹ï¼Œè¿™ä¸ªåŠŸèƒ½æœ€å¥½å…³é—­ï¼Œèƒ½å¤Ÿåšåˆ°åœ¨ä¸€å¼€å§‹å°±è®¾è®¡å¥½å•ä¸ª bucket çš„ shard æ•°é‡ï¼Œä¸€æ­¥åˆ°ä½æ˜¯æœ€å¥½ã€‚è‡³äºå¦‚ä½•åšå¥½ä¸€æ­¥åˆ°ä½çš„è®¾è®¡å¯ä»¥çœ‹å…¬ä¼—å·ä¹‹å‰çš„æ–‡ç« ã€‚(ã€ŠRGW Bucket Shard è®¾è®¡ä¸ä¼˜åŒ–ã€‹ç³»åˆ—)
```plain
#æ³¨æ„ä¸‹é¢çš„æ“ä½œä¸€å®šè¦ç¡®ä¿å¯¹åº”çš„bucketç›¸å…³çš„æ“ä½œéƒ½å·²ç»å…¨éƒ¨åœæ­¢ï¼Œä¹‹åä½¿ç”¨ä¸‹é¢å‘½ä»¤å¤‡ä»½bucketçš„index

$ radosgw-admin bi list --bucket=<bucket_name> > <bucket_name>.list.backup

#é€šè¿‡ä¸‹é¢çš„å‘½ä»¤æ¢å¤æ•°æ®
radosgw-admin bi put --bucket=<bucket_name> < <bucket_name>.list.backup

#æŸ¥çœ‹bucketçš„index id
$ radosgw-admin bucket stats --bucket=bucket-maillist
{
    "bucket": "bucket-maillist",
    "pool": "default.rgw.buckets.data",
    "index_pool": "default.rgw.buckets.index",
    "id": "0a6967a5-2c76-427a-99c6-8a788ca25034.54133.1", #æ³¨æ„è¿™ä¸ªid
    "marker": "0a6967a5-2c76-427a-99c6-8a788ca25034.54133.1",
    "owner": "user",
    "ver": "0#1,1#1",
    "master_ver": "0#0,1#0",
    "mtime": "2017-08-23 13:42:59.007081",
    "max_marker": "0#,1#",
    "usage": {},
    "bucket_quota": {
        "enabled": false,
        "max_size_kb": -1,
        "max_objects": -1
    }
}


#Reshardå¯¹åº”bucketçš„indexæ“ä½œå¦‚ä¸‹:
#ä½¿ç”¨å‘½ä»¤å°†"bucket-maillist"çš„shardè°ƒæ•´ä¸º4ï¼Œæ³¨æ„å‘½ä»¤ä¼šè¾“å‡ºosdå’Œnewä¸¤ä¸ªbucketçš„instance id

$ radosgw-admin bucket reshard --bucket="bucket-maillist" --num-shards=4
*** NOTICE: operation will not remove old bucket index objects ***
***         these will need to be removed manually             ***
old bucket instance id: 0a6967a5-2c76-427a-99c6-8a788ca25034.54133.1
new bucket instance id: 0a6967a5-2c76-427a-99c6-8a788ca25034.54147.1
total entries: 3


#ä¹‹åä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤åˆ é™¤æ—§çš„instance id

$ radosgw-admin bi purge --bucket="bucket-maillist" --bucket-id=0a6967a5-2c76-427a-99c6-8a788ca25034.54133.1

#æŸ¥çœ‹æœ€ç»ˆç»“æœ
$ radosgw-admin bucket stats --bucket=bucket-maillist
{
    "bucket": "bucket-maillist",
    "pool": "default.rgw.buckets.data",
    "index_pool": "default.rgw.buckets.index",
    "id": "0a6967a5-2c76-427a-99c6-8a788ca25034.54147.1", #idå·²ç»å˜æ›´
    "marker": "0a6967a5-2c76-427a-99c6-8a788ca25034.54133.1",
    "owner": "user",
    "ver": "0#2,1#1,2#1,3#2",
    "master_ver": "0#0,1#0,2#0,3#0",
    "mtime": "2017-08-23 14:02:19.961205",
    "max_marker": "0#,1#,2#,3#",
    "usage": {
        "rgw.main": {
            "size_kb": 50,
            "size_kb_actual": 60,
            "num_objects": 3
        }
    },
    "bucket_quota": {
        "enabled": false,
        "max_size_kb": -1,
        "max_objects": -1
    }
}
```
## 4. æ€»ç»“

1.  å¦å¤–å¯ä»¥åšåˆ°çš„å°±æ˜¯å•ç‹¬ä½¿ç”¨ SSD æˆ–è€… NVME ä½œä¸º index pool çš„ OSDï¼Œä½†æ˜¯ Leveldb ä»è®¾è®¡ä¸Šå¯¹ SSD çš„æ”¯æŒæ¯”è¾ƒæœ‰é™ï¼Œæœ€å¥½èƒ½å¤Ÿåˆ‡æ¢åˆ° rocksdb ä¸Šé¢å»ï¼ŒåŒæ—¶åœ¨ jewel ä¹‹å‰çš„ç‰ˆæœ¬è¿˜ä¸æ”¯æŒåˆ‡æ¢ omap å¼•æ“åˆ° rocksdbï¼Œé™¤éæ‰“ä¸Šä¸‹é¢çš„è¡¥ä¸Â [https://github.com/ceph/ceph/pull/18010](https://github.com/ceph/ceph/pull/18010)

2.  bucket çš„ index shard æ•°é‡æå‰åšå¥½è§„åˆ’ï¼Œè¿™ä¸ªå¯ä»¥å‚è€ƒæœ¬å…¬ä¼—å·ä¹‹å‰çš„å‡ ç¯‡ bucket index shard ç›¸å…³å†…å®¹ã€‚

3.  jewel ä¹‹å‰çš„ç‰ˆæœ¬ LevelDB å¦‚æœç¡¬ä»¶æ¡ä»¶å…è®¸å¯ä»¥è€ƒè™‘åˆ‡æ¢åˆ° rocksdb åŒæ—¶è€ƒè™‘åœ¨ä¸šåŠ¡é«˜å³°æœŸå…³é—­ deep-scrubã€‚å¦‚æœæ˜¯æ–°ä¸Šçš„é›†ç¾¤ç”¨ L ç‰ˆæœ¬çš„ cephï¼Œæ”¾å¼ƒ Filestoreï¼ŒåŒæ—¶ä½¿ç”¨ Bluestore ä½œä¸ºé»˜è®¤çš„å­˜å‚¨å¼•æ“ã€‚

æ€»è€Œè¨€ä¹‹ bucket index çš„æ€§èƒ½éœ€è¦æœ‰ SSD åŠ æŒï¼Œå¤§è§„æ¨¡é›†ç¾¤ä¸€å®šè¦åšå¥½åˆæœŸè®¾è®¡ï¼Œç­‰åˆ°æ•°æ®é‡å¤§äº†å†åšè°ƒæ•´ï¼Œå¾ˆéš¾åšåˆ°äº¡ç¾Šè¡¥ç‰¢ï¼

**å‚è€ƒï¼š**
[Ceph äºšå¤ªå³°ä¼š RGW è®®é¢˜åˆ†äº«](https://mp.weixin.qq.com/s?__biz=MzUyMDAwNjYyMw==&mid=2247483875&idx=1&sn=ddb35db83e72af2b14516a0948087c6c&chksm=f9f1bf53ce8636456a3b866eb717c651e0a37ea32e649f5a53ac8e9af57390ead59803b76ba9&mpshare=1&scene=1&srcid=0806so7DCFK0o40LfIVjKtBo#rd)
[RGW Bucket Shard è®¾è®¡ä¸ä¼˜åŒ–](https://cloud.tencent.com/developer/article/1032854)
