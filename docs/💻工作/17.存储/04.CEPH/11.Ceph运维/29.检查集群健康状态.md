---
title: æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€

tags: ceph
categories: 
  - ğŸ’»å·¥ä½œ
  - å­˜å‚¨
  - CEPH
  - Ceph è¿ç»´
date: 2020-05-23 11:02:28
permalink: /pages/daa21b/
---
# 1. æ£€æŸ¥é›†ç¾¤
## 1.1 æ£€æŸ¥é›†ç¾¤çš„çŠ¶æ€
```plain
#ä¸»è¦æ£€æŸ¥é›†ç¾¤çŠ¶æ€æ˜¯å¦æ˜¯ HEALTH_OK

ceph -s
    cluster aa7e0345-87a9-4860-a6b5-3158fd00b5a9
     health HEALTH_WARN
            nodeep-scrub,sortbitwise flag(s) set
     monmap e3: 3 mons at {op-xxx-ceph00=10.69.1.1:6789/0,op-xxx-ceph01=10.69.1.2:6789/0,op-xxx-ceph02=10.69.1.3:6789/0}
            election epoch 264, quorum 0,1,2 op-xxx-ceph02,op-xxx-ceph00,op-xxx-ceph01
      fsmap e12557: 1/1/1 up {0=op-xxx-ceph05=up:active}, 1 up:standby
     osdmap e24858: 59 osds: 59 up, 59 in
            flags nodeep-scrub,sortbitwise
      pgmap v50468419: 28832 pgs, 14 pools, 34479 GB data, 41212 kobjects
            101 TB used, 112 TB / 214 TB avail
               28822 active+clean
                  10 active+clean+scrubbing
  client io 329 kB/s wr, 0 op/s rd, 51 op/s wr
```
**è¾“å‡ºä¿¡æ¯é‡ŒåŒ…å«ï¼š**
 - é›†ç¾¤çš„ ID
 - é›†ç¾¤å¥åº·çŠ¶å†µ
 - monitor map ç‰ˆæœ¬å’Œ mon æ³•å®šäººæ•°çŠ¶æ€
 - OSD map ç‰ˆæœ¬å’Œ OSD çŠ¶æ€æ‘˜è¦
 - PG map ç‰ˆæœ¬
 - PG å’Œ Pool çš„æ•°é‡
 - é›†ç¾¤å­˜å‚¨çš„æ•°æ®é‡ï¼Œå¯¹è±¡çš„æ€»é‡ï¼Œä»¥åŠé›†ç¾¤çš„å·²ç”¨å®¹é‡/æ€»å®¹é‡/å¯ç”¨
 - å®¹é‡
 - å®¢æˆ·ç«¯çš„ iops ä¿¡æ¯

## 1.2 æ£€æŸ¥é›†ç¾¤çš„å®¹é‡æƒ…å†µ
```plain
#æŸ¥çœ‹é›†ç¾¤å®¹é‡ä½¿ç”¨æƒ…å†µ %RAW USED

ceph df
GLOBAL:
    SIZE     AVAIL     RAW USED     %RAW USED
    214T      112T         101T         47.45
POOLS:
    NAME                          ID     USED       %USED     MAX AVAIL     OBJECTS
    rbd                           0         115         0        31508G            4
    cephfs_data                   1      31252G     42.67        31508G     13821502
    cephfs_metadata               2      46967k         0        31508G        31385
    .rgw.root                     5        1636         0        31508G            4
    default.rgw.control           6           0         0        31508G            8
    default.rgw.data.root         7        6892         0        31508G           22
    default.rgw.gc                8           0         0        31508G           32
    default.rgw.log               9           0         0        31508G          127
    default.rgw.users.uid         10       1492         0        31508G            6
    default.rgw.users.swift       11         12         0        31508G            1
    default.rgw.users.keys        12         51         0        31508G            4
    default.rgw.meta              13      28985         0        31508G           87
    default.rgw.buckets.index     14          0         0        31508G           26
    default.rgw.buckets.data      15      3226G      4.41        31508G     28351099
```
**è¾“å‡ºçš„ GLOBAL æ®µå±•ç¤ºäº†æ•°æ®æ‰€å ç”¨é›†ç¾¤å­˜å‚¨ç©ºé—´çš„æ¦‚è¦ï¼š**
 - SIZEï¼š é›†ç¾¤çš„æ€»å®¹é‡ã€‚
 - AVAILï¼š é›†ç¾¤çš„å¯ç”¨ç©ºé—´æ€»é‡ã€‚
 - RAW USEDï¼šå·²ç”¨å­˜å‚¨ç©ºé—´æ€»é‡ã€‚
 - % RAW USEDï¼šå·²ç”¨å­˜å‚¨ç©ºé—´æ¯”ç‡ã€‚ç”¨æ­¤å€¼å¯¹æ¯” full ratio å’Œ near full ratio æ¥ç¡®ä¿ä¸ä¼šç”¨å°½é›†ç¾¤ç©ºé—´ã€‚
è¾“å‡ºçš„ POOLS æ®µå±•ç¤ºäº†å­˜å‚¨æ± åˆ—è¡¨åŠå„å­˜å‚¨æ± çš„å¤§è‡´ä½¿ç”¨ç‡ã€‚æœ¬æ®µæ²¡æœ‰åæ˜ å‡ºå‰¯æœ¬ã€å…‹éš†å’Œå¿«ç…§çš„å ç”¨æƒ…å†µã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æŠŠ 1MB çš„æ•°æ®å­˜å‚¨ä¸ºå¯¹è±¡ï¼Œç†è®ºä½¿ç”¨ç‡å°†æ˜¯ 1MB ï¼Œä½†è€ƒè™‘åˆ°å‰¯æœ¬æ•°ã€å…‹éš†æ•°ã€å’Œå¿«ç…§æ•°ï¼Œå®é™…ä½¿ç”¨é‡å¯èƒ½æ˜¯ 2MB æˆ–æ›´å¤šã€‚
 - NAMEï¼šå­˜å‚¨æ± åå­—ã€‚
 - IDï¼šå­˜å‚¨æ± å”¯ä¸€æ ‡è¯†ç¬¦ã€‚
 - USEDï¼šå¤§æ¦‚æ•°æ®é‡ï¼Œå•ä½ä¸º KB ã€MB æˆ– GB ï¼›
 - %USEDï¼šå„å­˜å‚¨æ± çš„å¤§æ¦‚ä½¿ç”¨ç‡ã€‚
 - Objectsï¼šå„å­˜å‚¨æ± å†…çš„å¤§æ¦‚å¯¹è±¡æ•°ã€‚

# 2. æ£€æŸ¥ osd
## 2.1 æ£€æŸ¥ osd çŠ¶æ€
```plain
#æ‰€æœ‰OSDçŠ¶æ€ä¸ºupä¸”in
ceph osd stat
     osdmap e24858: 59 osds: 59 up, 59 in
```
## 2.2 æ£€æŸ¥ osd crushmap æ˜¯å¦ä¸€è‡´
```plain
#æ£€æŸ¥WEIGHTåˆ—ï¼Œä»¥åŠREWEIGHT æƒé‡æ˜¯å¦ä¸€è‡´

ID WEIGHT    TYPE NAME               UP/DOWN REWEIGHT PRIMARY-AFFINITY
-1 218.21495 root default
-2  43.64299     host op-xxx-ceph00
 0   3.63699         osd.0                up  1.00000          1.00000
 1   3.63699         osd.1                up  1.00000          1.00000
 2   3.63699         osd.2                up  1.00000          1.00000
 3   3.63699         osd.3                up  1.00000          1.00000
 4   3.63699         osd.4                up  1.00000          1.00000
 5   3.63699         osd.5                up  1.00000          1.00000
 6   3.63699         osd.6                up  1.00000          1.00000
 7   3.63699         osd.7                up  1.00000          1.00000
 8   3.63699         osd.8                up  1.00000          1.00000
 9   3.63699         osd.9                up  1.00000          1.00000
10   3.63699         osd.10               up  1.00000          1.00000
11   3.63699         osd.11               up  1.00000          1.00000
-3  43.64299     host op-xxx-ceph01
13   3.63699         osd.13               up  1.00000          1.00000
14   3.63699         osd.14               up  1.00000          1.00000
15   3.63699         osd.15               up  1.00000          1.00000
16   3.63699         osd.16               up  1.00000          1.00000
17   3.63699         osd.17               up  1.00000          1.00000
18   3.63699         osd.18               up  1.00000          1.00000
19   3.63699         osd.19               up  1.00000          1.00000
20   3.63699         osd.20               up  1.00000          1.00000
21   3.63699         osd.21               up  1.00000          1.00000
22   3.63699         osd.22               up  1.00000          1.00000
23   3.63699         osd.23               up  1.00000          1.00000
24   3.63699         osd.24               up  1.00000          1.00000
```
## 2.3 æ£€æŸ¥ osd å®¹é‡æ˜¯å¦å‡è¡¡
```plain
#1. æ£€æŸ¥%USE æŸ¥çœ‹æ¯ä¸ªosdå®¹é‡æƒ…å†µæ˜¯å¦ä¸€è‡´
#2. æ£€æŸ¥rackæœºæ¶ä¸‹é¢çš„hostæ•°é‡æ˜¯å¦å‡è¡¡

ceph osd df tree
ID   CLASS WEIGHT     REWEIGHT SIZE   USE    AVAIL  %USE VAR  PGS TYPE NAME
  -1       1394.89197        -  1394T 45797G  1350T 3.21 1.00   - root default
-118        348.72299        -   348T 11544G   337T 3.23 1.01   -     rack xxx-18
  -3         87.18100        - 89274G  2888G 86385G 3.24 1.01   -         host ceph-xxx-hdd00
   0   hdd    7.26500  1.00000  7439G   252G  7186G 3.40 1.06  81             osd.0
   1   hdd    7.26500  1.00000  7439G   235G  7203G 3.17 0.99  78             osd.1
   2   hdd    7.26500  1.00000  7439G   262G  7177G 3.52 1.10  94             osd.2
   3   hdd    7.26500  1.00000  7439G   284G  7155G 3.82 1.19 107             osd.3
   4   hdd    7.26500  1.00000  7439G   236G  7203G 3.18 0.99  85             osd.4
   5   hdd    7.26500  1.00000  7439G   245G  7193G 3.30 1.03  83             osd.5
   6   hdd    7.26500  1.00000  7439G   231G  7207G 3.11 0.97  91             osd.6
   7   hdd    7.26500  1.00000  7439G   257G  7181G 3.46 1.08 100             osd.7
   8   hdd    7.26500  1.00000  7439G   190G  7249G 2.56 0.80  75             osd.8
   9   hdd    7.26500  1.00000  7439G   212G  7227G 2.85 0.89  79             osd.9
  10   hdd    7.26500  1.00000  7439G   248G  7190G 3.34 1.04  88             osd.10
  11   hdd    7.26500  1.00000  7439G   231G  7207G 3.11 0.97 102             osd.11
-119        348.72299        -   348T 11196G   337T 3.14 0.98   -     rack xxx-19
 -11         87.18100        - 89274G  2830G 86443G 3.17 0.99   -         host ceph-xxx-hdd04
  48   hdd    7.26500  1.00000  7439G   223G  7215G 3.01 0.94  78             osd.48
  49   hdd    7.26500  1.00000  7439G   279G  7160G 3.75 1.17  99             osd.49
  50   hdd    7.26500  1.00000  7439G   198G  7241G 2.66 0.83  76             osd.50
  51   hdd    7.26500  1.00000  7439G   230G  7209G 3.09 0.97  87             osd.51
  52   hdd    7.26500  1.00000  7439G   257G  7181G 3.46 1.08  89             osd.52
  53   hdd    7.26500  1.00000  7439G   193G  7246G 2.60 0.81  76             osd.53
  54   hdd    7.26500  1.00000  7439G   222G  7216G 2.99 0.93  81             osd.54
  55   hdd    7.26500  1.00000  7439G   237G  7201G 3.20 1.00  90             osd.55
  56   hdd    7.26500  1.00000  7439G   234G  7205G 3.15 0.98  85             osd.56
  57   hdd    7.26500  1.00000  7439G   251G  7187G 3.38 1.05 104             osd.57
  58   hdd    7.26500  1.00000  7439G   244G  7195G 3.28 1.02  88             osd.58
  59   hdd    7.26500  1.00000  7439G   257G  7182G 3.46 1.08  98             osd.59
-120        348.72299        -   348T 11582G   337T 3.24 1.01   -     rack xxx-20
  -5         87.18100        - 89274G  3066G 86207G 3.43 1.07   -         host ceph-xxx-hdd01
  12   hdd    7.26500  1.00000  7439G   302G  7137G 4.07 1.27 104             osd.12
  13   hdd    7.26500  1.00000  7439G   241G  7197G 3.25 1.01 105             osd.13
  14   hdd    7.26500  1.00000  7439G   318G  7120G 4.29 1.34 123             osd.14
  15   hdd    7.26500  1.00000  7439G   229G  7210G 3.08 0.96  74             osd.15
  16   hdd    7.26500  1.00000  7439G   272G  7167G 3.66 1.14 101             osd.16
  17   hdd    7.26500  1.00000  7439G   264G  7174G 3.56 1.11  94             osd.17
  18   hdd    7.26500  1.00000  7439G   241G  7197G 3.25 1.01  93             osd.18
  19   hdd    7.26500  1.00000  7439G   245G  7193G 3.30 1.03  98             osd.19
  20   hdd    7.26500  1.00000  7439G   211G  7227G 2.84 0.89  74             osd.20
  21   hdd    7.26500  1.00000  7439G   264G  7174G 3.56 1.11  92             osd.21
  22   hdd    7.26500  1.00000  7439G   264G  7175G 3.55 1.11  93             osd.22
  23   hdd    7.26500  1.00000  7439G   208G  7230G 2.81 0.88  83             osd.23
-121        348.72299        -   348T 11474G   337T 3.21 1.00   -     rack xxx-21
  -7         87.18100        - 89274G  2844G 86429G 3.19 0.99   -         host ceph-xxx-hdd02
  24   hdd    7.26500  1.00000  7439G   212G  7226G 2.86 0.89  77             osd.24
  25   hdd    7.26500  1.00000  7439G   258G  7180G 3.48 1.08  85             osd.25
  26   hdd    7.26500  1.00000  7439G   223G  7216G 3.00 0.94  83             osd.26
  27   hdd    7.26500  1.00000  7439G   216G  7222G 2.91 0.91  86             osd.27
  28   hdd    7.26500  1.00000  7439G   252G  7187G 3.39 1.06  85             osd.28
  29   hdd    7.26500  1.00000  7439G   190G  7249G 2.56 0.80  82             osd.29
  30   hdd    7.26500  1.00000  7439G   255G  7183G 3.44 1.07  92             osd.30
  31   hdd    7.26500  1.00000  7439G   260G  7178G 3.51 1.09  90             osd.31
  32   hdd    7.26500  1.00000  7439G   241G  7198G 3.24 1.01  95             osd.32
  33   hdd    7.26500  1.00000  7439G   252G  7187G 3.39 1.06 102             osd.33
  34   hdd    7.26500  1.00000  7439G   279G  7160G 3.75 1.17  92             osd.34
  35   hdd    7.26500  1.00000  7439G   201G  7237G 2.71 0.85  79             osd.35
```
## 2.4 æ£€æŸ¥ osd ä¸‰å‰¯æœ¬æ˜¯å¦åˆ†æ•£æœºæ¶
```plain
#æŠ½æŸ¥osd[31,38,1] æ£€æŸ¥31ï¼Œ38ï¼Œ1æ˜¯å¦åˆ†å¼€å¤šä¸ªrackï¼Œå¦‚æœæ²¡æœ‰æœºæ¶åˆ™åˆ†æ•£å¤šä¸ªhost

ceph pg dump pgs|grep ^1|awk 'BEGIN {print "PG \t\t OBJECTS \t\t BYTES \t\tosd"} {print $1 "\t\t" $2 "\t\t" $7 "\t\t" $15}' | head
dumped pgs in format plain
PG       OBJECTS         BYTES      osd
1.f9f       3339        8198351373      [31,38,1]
1.f9e       3352        8060814080      [48,7,52]
1.f9d       3484        8460769474      [32,3,18]
1.f9c       3283        8113961689      [0,60,26]
1.f9b       3408        7863461676      [60,14,42]
1.f9a       3454        8449437644      [42,7,19]
1.f99       3326        8301474267      [8,18,39]
1.f98       3414        8259614607      [52,30,14]
1.f97       3373        8180890502      [2,19,52]


#ä¸‰å‰¯æœ¬å…¨é‡æ£€æŸ¥æ˜¯å¦åˆ†é…åœ¨å¤šæœºæ¶
master=""
slave1=""
slave2=""
ceph pg dump pgs|grep ^1|awk ' {print $15}' > pgs.log
sed -i "s/\[//g" pgs.log
sed -i "s/\]//g" pgs.log
for line in `cat pgs.log`
do
    master_num=`echo $line | cut -d "," -f 1`
    slave1_num=`echo $line | cut -d "," -f 2`
    slave2_num=`echo $line | cut -d "," -f 3`

    master=`ceph osd tree | grep  "osd.$master_num" -B 12 | grep host | tail -1 | awk -F  ' ' '{print $4}'`
    slave1=`ceph osd tree | grep  "osd.$slave1_num" -B 12 | grep host | tail -1 | awk -F  ' ' '{print $4}'`
    slave2=`ceph osd tree | grep  "osd.$slave2_num" -B 12 | grep host | tail -1 | awk -F  ' ' '{print $4}'`
    echo " master:" $master " slave1 " $slave1 " slave2 " $slave2
    master_ip=`host $master | awk -F ' ' '{print $4}' | cut -d "." -f 1,2`
    slave1_ip=`host $slave1 | awk -F ' ' '{print $4}' | cut -d "." -f 1,2`
    slave2_ip=`host $slave2 | awk -F ' ' '{print $4}' | cut -d "." -f 1,2`
    echo " master:" $master_ip " slave1 " $slave1_ip " slave2 " $slave2_ip
    if [[ "$master_ip" = "$slave1_ip" ]] || [[ "$slave1_ip" = "$slave2_ip" ]] || [[ "$master_ip" = "$slave2_ip" ]]; then
        echo "failed"
    break
    else
        echo "ok"
    fi
done
```
## 2.5 æ£€æŸ¥æ¢å¤é™é€Ÿé…ç½®
```plain
#æ£€æŸ¥æ¢å¤é™é€Ÿå‚æ•°æ˜¯å¦ç”Ÿæ•ˆ
#osd recovery max active = 3 ï¼ˆdefault : 15)
#osd recovery op priority = 3 (default : 10)
#osd max backfills = 1 (default : 10)

ceph daemon osd.6 config show | grep  -E 'osd_recovery_max_active|osd_recovery_op_priority|osd_max_backfills'
"osd_max_backfills": "1",
"osd_recovery_max_active": "1",
"osd_recovery_op_priority": "1",
```
## 2.6 æ£€æŸ¥æ•°æ®æ¸…æ´—æ—¶é—´æ®µ
```plain
#æ£€æŸ¥æ¢å¤é™é€Ÿå‚æ•°æ˜¯å¦ç”Ÿæ•ˆ,å‡Œæ™¨0ç‚¹-5ç‚¹
#osd_scrub_begin_hour = 0
#osd_scrub_end_hour = 5

ceph daemon osd.6 config show | grep  -E 'osd_scrub_begin_hour|osd_scrub_end_hour'
"osd_scrub_begin_hour": "0",
"osd_scrub_end_hour": "5",
```
# 3. æ£€æŸ¥ mon
## 3.1 æ£€æŸ¥ mon çŠ¶æ€
```plain
#æ£€æŸ¥monæ˜¯å¦éƒ½æ˜¯åœ¨çº¿

ceph mon stat
e3: 3 mons at {op-xxx-ceph00=10.9.1.1:6789/0,op-xxx-ceph01=10.9.1.2:6789/0,op-xxx-ceph02=10.9.1.3:6789/0}, election epoch 264, quorum 0,1,2 op-xxx-ceph02,op-xxx-ceph00,op-xxx-ceph01
```
## 3.2 æ£€æŸ¥ mon æœºæ¶åˆ†å¸ƒ
```plain
#æ£€æŸ¥monæ˜¯å¦åˆ†å¸ƒåœ¨å¤šä¸ªæœºæ¶

ceph mon dump
dumped monmap epoch 3
epoch 3
fsid aa7e0345-87a9-4860-a6b5-3158fd00b5a9
last_changed 2016-07-27 16:51:39.275244
created 2016-07-27 02:42:27.838682
0: 10.9.1.1:6789/0 mon.op-xxx-ceph02
1: 10.9.1.2:6789/0 mon.op-xxx-ceph00
2: 10.9.1.3:6789/0 mon.op-xxx-ceph01
```
# 4. æ£€æŸ¥ MDS
## 4.1 æ£€æŸ¥ MDS çŠ¶æ€
å…ƒæ•°æ®æœåŠ¡å™¨ä¸º Ceph æ–‡ä»¶ç³»ç»Ÿæä¾›å…ƒæ•°æ®æœåŠ¡ï¼Œä¸è¿‡åœ¨å½“å‰ç”Ÿäº§ç¯å¢ƒä¸­å¹¶æœªéƒ¨ç½² MDS ã€‚

å…ƒæ•°æ®æœåŠ¡å™¨æœ‰ä¸¤ç§çŠ¶æ€ï¼š up | down å’Œ active | inactive ï¼Œæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤æŸ¥çœ‹å…ƒæ•°æ®æœåŠ¡å™¨çŠ¶æ€ä¸º up ä¸” active ï¼š
```plain
ceph mds stat
e12557: 1/1/1 up {0=op-xxx-ceph05=up:active}, 1 up:standby
```
# 5. æ£€æŸ¥ PG
## 5.1 æ£€æŸ¥ PG çŠ¶æ€
PG æŠŠå¯¹è±¡æ˜ å°„åˆ° OSD ã€‚ç›‘æ§ PG æ—¶ï¼Œæˆ‘ä»¬å¸Œæœ›å®ƒä»¬çš„çŠ¶æ€æ˜¯ active ä¸” cleanã€‚
```plain
ceph pg stat
v50681978: 28832 pgs: 9 active+clean+scrubbing, 28823 active+clean; 34497 GB data, 101 TB used, 112 TB / 214 TB avail; 198 kB/s rd, 182 kB/s wr, 55 op/s
```

