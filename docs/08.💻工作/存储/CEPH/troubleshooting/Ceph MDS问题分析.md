---
title: Ceph MDS é—®é¢˜åˆ†æ

categories: 
  - ğŸ’» å·¥ä½œ
  - å­˜å‚¨
  - CEPH
  - troubleshooting
date: 2020-05-23 12:27:56
tags: null
permalink: /pages/ea258b/
---
# 1. é—®é¢˜èƒŒæ™¯
## 1.1 å®¢æˆ·ç«¯ç¼“å­˜é—®é¢˜
```shell
$ ceph -s
health HEALTH_WARN
mds0: Client xxx-online00.gz01 failing to respond to cache pressure
```
**å®˜æ–¹è§£é‡Š**

**æ¶ˆæ¯:**	â€œClient name failing to respond to cache pressureâ€
**ä»£ç :**	 MDS_HEALTH_CLIENT_RECALL, MDS_HEALTH_CLIENT_RECALL_MANY
**æè¿°:**
å®¢æˆ·ç«¯æœ‰å„è‡ªçš„å…ƒæ•°æ®ç¼“å­˜ï¼Œå®¢æˆ·ç«¯ç¼“å­˜ä¸­çš„æ¡ç›®ï¼ˆæ¯”å¦‚ç´¢å¼•èŠ‚ç‚¹ï¼‰ä¹Ÿä¼šå­˜åœ¨äº MDS ç¼“å­˜ä¸­ï¼Œæ‰€ä»¥å½“ MDS éœ€è¦å‰Šå‡å…¶ç¼“å­˜æ—¶ï¼ˆä¿æŒåœ¨ mds_cache_size ä»¥ä¸‹ï¼‰ï¼Œå®ƒä¹Ÿä¼šå‘æ¶ˆæ¯ç»™å®¢æˆ·ç«¯è®©å®ƒä»¬å‰Šå‡è‡ªå·±çš„ç¼“å­˜ã€‚å¦‚æœæœ‰å®¢æˆ·ç«¯æ²¡å“åº”æˆ–è€…æœ‰ç¼ºé™·ï¼Œå°±ä¼šå¦¨ç¢ MDS å°†ç¼“å­˜ä¿æŒåœ¨ mds_cache_size ä»¥ä¸‹ï¼Œ MDS å°±æœ‰å¯èƒ½è€—å°½å†…å­˜è€Œåå´©æºƒã€‚å¦‚æœæŸä¸ªå®¢æˆ·ç«¯çš„å“åº”æ—¶é—´è¶…è¿‡äº† mds_recall_state_timeout ï¼ˆé»˜è®¤ä¸º 60s ï¼‰ï¼Œè¿™æ¡æ¶ˆæ¯å°±ä¼šå‡ºç°ã€‚

## 1.2 æœåŠ¡ç«¯å†…å­˜ä¸é‡Šæ”¾
åŒä¸Šå‚è€ƒ 1.1 å®¢æˆ·ç«¯ç¼“å­˜é—®é¢˜

## 1.3 mds session çš„ inode è¿‡å¤š
å®¢æˆ·ç«¯ session çš„ inode å¤ªå¤šï¼Œå¯¼è‡´å†…å­˜å¾ˆé«˜ï¼Œä»è€Œä¹Ÿå¯¼è‡´ä¸»ä» mds åˆ‡æ¢åŠ è½½ inode æ…¢ï¼Œä¸¥é‡å½±å“æœåŠ¡çš„å¯ç”¨æ€§ã€‚

## 1.4 mds å¤¯ä½é—®é¢˜æˆ–æ…¢æŸ¥è¯¢
 - å®¢æˆ·ç«¯æœç´¢éå†æŸ¥æ‰¾æ–‡ä»¶ï¼ˆä¸å¯æ§)
 - session çš„ inode å¤ªå¤§å¯¼è‡´ mds è´Ÿè½½è¿‡é«˜
 - æ—¥å¿—çº§åˆ«å¼€çš„å¤ªå¤§ï¼Œä»è€Œå¯¼è‡´ mds è´Ÿè½½é«˜

# 2. åˆ†ææ€è·¯
ä¸Šé¢çš„å‡ ä¸ªé—®é¢˜éƒ½æ˜¯æœ‰ä¸€å®šçš„è”ç³»ï¼Œäº’ç›¸å½±å“çš„ã€‚æ‰€ä»¥ï¼Œæˆ‘ä»¬å…ˆä»å·²çŸ¥çš„æ–¹å‘é€æ­¥æ·±å…¥åˆ†æé—®é¢˜ï¼Œä»è€Œä¼˜åŒ–è§£å†³é—®é¢˜ã€‚

## 2.1 ç»„ä»¶é€šä¿¡æµç¨‹å›¾
![image.png](https://upload-images.jianshu.io/upload_images/2099201-3500c1b07249921b.png)

1. Client <â€“> MDS
    å…ƒæ•°æ®æ“ä½œå’Œ capalities
2. Client <â€“> OSD
    æ•°æ® IO
3. Client <â€“> Monitor
    è®¤è¯ï¼Œé›†ç¾¤ map ä¿¡æ¯ç­‰
4. MDS <â€“> Monitor
    å¿ƒè·³ï¼Œé›†ç¾¤ map ä¿¡æ¯ç­‰
5. MDS <â€“> OSD
    å…ƒæ•°æ® IO
6. Monitor <â€“> OSD
    å¿ƒè·³ï¼Œé›†ç¾¤ map ä¿¡æ¯ç­‰

## 2.2 æŸ¥çœ‹å®¢æˆ·ç«¯ session
```shell
$ ceph --admin-daemon  /var/run/ceph/ceph-mds.ceph-xxxx-osd02.py.asok session ls
[
    {
        "id": 5122511,
        "num_leases": 0,
        "num_caps": 655,
        "state": "open",
        "replay_requests": 0,
        "completed_requests": 1,
        "reconnecting": false,
        "inst": "client.5122511 192.168.1.2:0\/2026289820",
        "client_metadata": {
            "ceph_sha1": "b1e0532418e4631af01acbc0cedd426f1905f4af",
            "ceph_version": "ceph version 0.94.10 (b1e0532418e4631af01acbc0cedd426f1905f4af)",
            "entity_id": "log_xxx_cephfs",
            "hostname": "ceph-test-osd02",
            "mount_point": "\/mnt\/log"
        }
    }
]
```
è¯´æ˜ï¼š
 - idï¼šclient å”¯ä¸€ id
 - num_capsï¼šclient è·å–çš„ caps
 - instï¼šclient ç«¯çš„ ip å’Œç«¯å£é“¾æ¥ä¿¡æ¯
 - ceph_versionï¼šclient ç«¯çš„ ceph-fuse ç‰ˆæœ¬ï¼Œè‹¥ä½¿ç”¨ kernel clientï¼Œåˆ™ä¸º kernel_version
 - hostnameï¼šclient ç«¯çš„ä¸»æœºå
 - mount_pointï¼šclient åœ¨ä¸»æœºä¸Šå¯¹åº”çš„ mount point
 - pidï¼šclient ç«¯ ceph-fuse è¿›ç¨‹çš„ pid

## 2.3 æŸ¥çœ‹å®¢æˆ·ç«¯çš„ inode æ•°é‡
è·Ÿè¸ªä»£ç å‘ç° session é‡Œé¢çš„ num_caps å°±æ˜¯ç»Ÿè®¡çš„å®¢æˆ·ç«¯çš„ inode æ•°é‡, å¤§æ¦‚ç»Ÿè®¡äº†ä¸‹å·²ç»æ‰“å¼€çš„ inode æ•°é‡åœ¨ 400w å·¦å³ã€‚
![image.png](https://upload-images.jianshu.io/upload_images/2099201-f527b758968dc069.png)

**æ€»ç»“ï¼š**
å¯ä»¥æŸ¥çœ‹å®¢æˆ·ç«¯çš„ session ä¿¡æ¯ï¼ŒåŒ…å« hostã€mountã€inode ç­‰ä¿¡æ¯
å¯ä»¥ç»Ÿè®¡æ‰€æœ‰å®¢æˆ·ç«¯ session çš„ inode æ•°é‡ã€‚

## 2.4  å°è¯• mds ä¸»ä»åˆ‡æ¢
### 2.4.1 æ‰§è¡Œè¿‡ç¨‹å¦‚ä¸‹

```shell
2018-04-27 19:24:03.923349 7f53015d7700  1 mds.0.2738 handle_mds_map state change up:boot --> up:replay
2018-04-27 19:24:03.923356 7f53015d7700  1 mds.0.2738 replay_start
2018-04-27 19:24:03.923360 7f53015d7700  1 mds.0.2738  recovery set is
2018-04-27 19:24:03.923365 7f53015d7700  1 mds.0.2738  waiting for osdmap 6339 (which blacklists prior instance)
2018-04-27 19:24:03.948526 7f52fc2ca700  0 mds.0.cache creating system inode with ino:100
2018-04-27 19:24:03.948675 7f52fc2ca700  0 mds.0.cache creating system inode with ino:1
2018-04-27 19:24:04.238128 7f52fa2b8700  1 mds.0.2738 replay_done
2018-04-27 19:24:04.238143 7f52fa2b8700  1 mds.0.2738 making mds journal writeable
2018-04-27 19:24:04.924352 7f53015d7700  1 mds.0.2738 handle_mds_map i am now mds.0.2738
2018-04-27 19:24:04.924357 7f53015d7700  1 mds.0.2738 handle_mds_map state change up:replay --> up:reconnect
2018-04-27 19:24:04.924370 7f53015d7700  1 mds.0.2738 reconnect_start
2018-04-27 19:24:04.924371 7f53015d7700  1 mds.0.2738 reopen_log
2018-04-27 19:24:04.924380 7f53015d7700  1 mds.0.server reconnect_clients -- 19 sessions
2018-04-27 19:24:04.926357 7f53015d7700  0 log_channel(cluster) log [DBG] : reconnect by client.4375 192.168.1.3:0/1796553051 after 0.001950
2018-04-27 19:24:04.926429 7f53015d7700  0 log_channel(cluster) log [DBG] : reconnect by client.4403 192.168.1.3:0/1032897847 after 0.002036
2018-04-27 19:24:15.228507 7f53015d7700  1 mds.0.2738 reconnect_done
2018-04-27 19:24:15.984143 7f53015d7700  1 mds.0.2738 handle_mds_map i am now mds.0.2738
2018-04-27 19:24:15.984148 7f53015d7700  1 mds.0.2738 handle_mds_map state change up:reconnect --> up:rejoin
2018-04-27 19:24:15.984156 7f53015d7700  1 mds.0.2738 rejoin_start
2018-04-27 19:25:15.987531 7f53015d7700  1 mds.0.2738 rejoin_joint_start
2018-04-27 19:27:40.105134 7f52fd4ce700  1 mds.0.2738 rejoin_done
2018-04-27 19:27:42.206654 7f53015d7700  1 mds.0.2738 handle_mds_map i am now mds.0.2738
2018-04-27 19:27:42.206658 7f53015d7700  1 mds.0.2738 handle_mds_map state change up:rejoin --> up:active
2018-04-27 19:27:42.206666 7f53015d7700  1 mds.0.2738 recovery_done -- successful recovery!
```

**ä¸»ä»åˆ‡æ¢æµç¨‹ï¼š**
 - handle_mds_map state change up:boot --> up:replay
 - handle_mds_map state change up:replay --> up:reconnect
 - handle_mds_map state change up:reconnect --> up:rejoin
 - handle_mds_map state change up:rejoin --> up:active

**up:boot**
   - æ­¤çŠ¶æ€åœ¨å¯åŠ¨æœŸé—´è¢«å¹¿æ’­åˆ° CEPH ç›‘è§†å™¨ã€‚è¿™ç§çŠ¶æ€æ˜¯ä¸å¯è§çš„ï¼Œå› ä¸ºç›‘è§†å™¨ç«‹å³å°† MDS åˆ†é…ç»™å¯ç”¨çš„ç§©æˆ–å‘½ä»¤ MDS ä½œä¸ºå¤‡ç”¨æ“ä½œã€‚è¿™é‡Œè®°å½•äº†å®Œæ•´æ€§çš„çŠ¶æ€ã€‚

**up:replay**
  - æ—¥å¿—æ¢å¤é˜¶æ®µï¼Œä»–å°†æ—¥å¿—å†…å®¹è¯»å…¥å†…å­˜åï¼Œåœ¨å†…å­˜ä¸­è¿›è¡Œå›æ”¾æ“ä½œã€‚

**up:reconnect**
   - æ¢å¤çš„ mds éœ€è¦ä¸ä¹‹å‰çš„å®¢æˆ·ç«¯é‡æ–°å»ºç«‹è¿æ¥ï¼Œå¹¶ä¸”éœ€è¦æŸ¥è¯¢ä¹‹å‰å®¢æˆ·ç«¯å‘å¸ƒçš„æ–‡ä»¶å¥æŸ„ï¼Œé‡æ–°åœ¨ mds çš„ç¼“å­˜ä¸­åˆ›å»ºä¸€è‡´æ€§åŠŸèƒ½å’Œé”çš„çŠ¶æ€ã€‚
mds ä¸ä¼šåŒæ­¥è®°å½•æ–‡ä»¶æ‰“å¼€çš„ä¿¡æ¯ï¼ŒåŸå› æ˜¯éœ€è¦é¿å…åœ¨è®¿é—® mds æ—¶äº§ç”Ÿå¤šä½™çš„å»¶è¿Ÿï¼Œå¹¶ä¸”å¤§å¤šæ•°æ–‡ä»¶æ˜¯ä»¥åªè¯»æ–¹å¼æ‰“å¼€ã€‚

**up:rejoin**
   - æŠŠå®¢æˆ·ç«¯çš„ inode åŠ è½½åˆ° mds cacheã€‚(è€—æ—¶æœ€å¤šçš„åœ°æ–¹)

**ä¸ºä»€ä¹ˆ mds åˆ‡æ¢è€—æ—¶æ¯”è¾ƒé«˜ï¼Ÿ**
 1. åˆ†ææ—¥å¿—(å‘ç°æ‰§è¡Œ rejoin_startï¼Œrejoin_joint_start åŠ¨ä½œè€—æ—¶æ¯”è¾ƒé«˜)
```plain
2018-04-27 19:24:15.984156 7f53015d7700  1 mds.0.2738 rejoin_start
2018-04-27 19:25:15.987531 7f53015d7700  1 mds.0.2738 rejoin_joint_start
2018-04-27 19:27:40.105134 7f52fd4ce700  1 mds.0.2738 rejoin_done
2018-04-27 19:27:42.206654 7f53015d7700  1 mds.0.2738 handle_mds_map i am now mds.0.2738
2018-04-27 19:27:42.206658 7f53015d7700  1 mds.0.2738 handle_mds_map state change up:rejoin --> up:active
```
 2. è·Ÿè¸ªä»£ç åˆ†æ(åœ¨æ‰§è¡Œ process_imported_caps è¶…æ—¶äº†ï¼Œ è¿™ä¸ªå‡½æ•°ä¸»è¦æ˜¯æ‰“å¼€ inodes åŠ è½½åˆ° cache ä¸­)
![image.png](https://upload-images.jianshu.io/upload_images/2099201-4b812491189dc713.png)

**æ€»ç»“ï¼š**
 - ä¸»ä»åˆ‡æ¢æ—¶ mds è¯¦ç»†çŠ¶æ€
 - ä¸»ä»åˆ‡æ¢æ—¶ä¸»è¦è€—æ—¶çš„é˜¶æ®µ rejoin_start,åŠ è½½å®¢æˆ·ç«¯ session çš„ inode ä¿¡æ¯

## 2.5  é‡Šæ”¾å®¢æˆ·ç«¯ inode
### 2.5.1 æ¨¡æ‹Ÿå®¢æˆ·ç«¯ session inode è¿‡å¤š
 1. æŸ¥çœ‹å®¢æˆ·ç«¯ session ä¿¡æ¯

```shell
#æŸ¥çœ‹å®¢æˆ·ç«¯sessionçš„inodeæ•°é‡ï¼Œ num_caps:7
$ ceph daemon mds.ceph-xxx-osd01.ys session ls
[
    {
        "id": 554418,
        "num_leases": 0,
        "num_caps": 7,
        "state": "open",
        "replay_requests": 0,
        "completed_requests": 0,
        "reconnecting": false,
        "inst": "client.554418 192.168.1.2:0/1285681097",
        "client_metadata": {
            "ceph_sha1": "fe3a2269d799a8b950404cb2de11af84c7af0ea4",
            "ceph_version": "didi_dss version 12.2.2.4 (fe3a2269d799a8b950404cb2de11af84c7af0ea4) luminous (stable)",
            "entity_id": "admin",
            "hostname": "ceph-xxx-osd01.ys",
            "mount_point": "/mnt",
            "pid": "2084",
            "root": "/"
        }
    }
]
```
 2. å®¢æˆ·ç«¯éå†æ‰€æœ‰æ–‡ä»¶
```shell
#éå†æŒ‚è½½ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶
$ tree /mnt/

#æŸ¥çœ‹è¿™ä¸ªç›®å½•ä¸‹é¢æ‰€æœ‰æ–‡ä»¶å¤¹åŠæ–‡ä»¶æ•°é‡
$ tree /mnt/ | wc -l
347
```
 3. å†æ¬¡æŸ¥çœ‹å®¢æˆ·ç«¯ session ä¿¡æ¯
```shell
#æŸ¥çœ‹å®¢æˆ·ç«¯sessionçš„inodeæ•°é‡ï¼Œ num_caps:346
$ ceph daemon mds.ceph-xxx-osd01.ys session ls
[
   {
        "id": 554418,
        "num_leases": 1,
        "num_caps": 346,
        "state": "open",
        "replay_requests": 0,
        "completed_requests": 2,
        "reconnecting": false,
        "inst": "client.554418 192.168.1.3:0/1285681097",
        "client_metadata": {
            "ceph_sha1": "fe3a2269d799a8b950404cb2de11af84c7af0ea4",
            "ceph_version": "didi_dss version 12.2.2.4 (fe3a2269d799a8b950404cb2de11af84c7af0ea4) luminous (stable)",
            "entity_id": "admin",
            "hostname": "ceph-xxx-osd01.ys",
            "mount_point": "/mnt",
            "pid": "2084",
            "root": "/"
        }
    }
]
```
**ç»“è®ºï¼š**
 - å®¢æˆ·ç«¯é€šè¿‡éå†æŒ‚è½½ç›®å½•ä¸‹æ‰€æœ‰æ–‡ä»¶ï¼Œå‘ç°æœåŠ¡ç«¯çš„ session num_caps è·Ÿå®¢æˆ·ç«¯æ–‡ä»¶å¤¹åŠæ–‡ä»¶æ¢³ç†åŒ¹é…
 - ä¹Ÿå°±æ˜¯è¯´å®¢æˆ·ç«¯è¯»å–è¿‡çš„æ–‡ä»¶å¥æŸ„ï¼Œéƒ½ä¼šåœ¨æœåŠ¡ç«¯è®°å½•ä¸‹æ¥ã€‚ (mds ç¼“å­˜äº† dentryï¼Œå¹¶ä¸”ä»¥ lru ç®—æ³•çš„ç¼“å­˜æ·˜æ±°æ–¹å¼æŠŠ dentry ç¼“å­˜åœ¨äº†å†…å­˜ä¸­)

### 2.5.2 é‡Šæ”¾å®¢æˆ·ç«¯ session inode
**è§£å†³æ–¹æ¡ˆï¼š**
 - æ–¹æ¡ˆ 1ï¼šé‡‡ç”¨å¤šæ´» mds(ç›®å‰ 12 ç‰ˆ multi active ä¸ç¨³å®š)
 - æ–¹æ¡ˆ 2ï¼ševict client(ä¸»åŠ¨è¸¢å‡ºæœ‰é—®é¢˜çš„å®¢æˆ·ç«¯)
 - æ–¹æ¡ˆ 3ï¼šclient remount(æœ‰é—®é¢˜çš„å®¢æˆ·ç«¯é‡æ–° mount æŒ‚è½½)
 - æ–¹æ¡ˆ 4ï¼šdrop_cache, limit_cache
   - mds limiting cache by memoryÂ [https://github.com/ceph/ceph/pull/17711](https://github.com/ceph/ceph/pull/17711)
   - (å®˜æ–¹æä¾›çš„ mds ä¸»åŠ¨åˆ é™¤ cacheï¼Œè¡¥ä¸åœ¨ review è¿‡ç¨‹ä¸­ä¸ªï¼Œç›®æ ‡ç‰ˆæœ¬æ˜¯ ceph-14.0.0)Â [https://github.com/ceph/ceph/pull/21566](https://github.com/ceph/ceph/pull/21566)
![image.png](https://upload-images.jianshu.io/upload_images/2099201-8df0da43251b9c08.png)

# 3. æ·±å…¥åˆ†æ
æ ¹æ®ä¸Šé¢çš„åˆ†æï¼Œæˆ‘ä»¬åŸºæœ¬æœ‰ä¸€å®šçš„æ€è·¯ã€‚ è¿™é‡Œæˆ‘ä»¬ç»§ç»­æ·±å…¥åˆ°æ–¹æ¡ˆ 2 ä¸­ã€‚

## 3.1 å‰”é™¤å®¢æˆ·ç«¯ session
### 3.1.1 æŸ¥çœ‹å®¢æˆ·ç«¯ session ä¿¡æ¯

```shell
$ ceph daemon mds.ceph-xxx-osd01.ys session ls
[
    {
        "id": 554418,
        "num_leases": 0,
        "num_caps": 1589,
        "state": "open",
        "replay_requests": 0,
        "completed_requests": 2,
        "reconnecting": false,
        "inst": "client.554418 192.168.1.2:0/1285681097",
        "client_metadata": {
            "ceph_sha1": "fe3a2269d799a8b950404cb2de11af84c7af0ea4",
            "ceph_version": "didi_dss version 12.2.2.4 (fe3a2269d799a8b950404cb2de11af84c7af0ea4) luminous (stable)",
            "entity_id": "admin",
            "hostname": "ceph-xxx-osd01.ys",
            "mount_point": "/mnt",
            "pid": "2084",
            "root": "/"
        }
    }
]
```
### 3.1.2 å‰”é™¤å®¢æˆ·ç«¯ session ä¿¡æ¯
```shell
$ ceph tell mds.ceph-xxx-osd01.ys client evict id=554418
```
### 3.1.3 æŸ¥çœ‹ osd çš„ blacklist
```shell
#è¶…æ—¶æ¢å¤çš„æ—¶é—´æ˜¯1å°æ—¶ï¼Œå‰”é™¤çš„æ—¶é—´æ˜¯16:16:30, æ¢å¤çš„æ—¶é—´æ˜¯17:16:30
$ ceph osd blacklist ls
listed 1 entries
192.168.1.2:0/1285681097 2018-10-10 17:16:30.819201
```
### 3.1.4 æŸ¥çœ‹å®¢æˆ·ç«¯æŒ‚è½½ç›®å½•(ä¸èƒ½è¯»å†™)
```shell
$ ll /mnt
ls: cannot access /mnt: No such file or directory
```
### 3.1.5 æ¢å¤å‰”é™¤çš„å®¢æˆ·ç«¯
```shell
$ ceph osd blacklist rm 192.168.1.2:0/1285681097
un-blacklisting 192.168.1.2:0/1285681097
```
### 3.1.6 æŸ¥çœ‹å®¢æˆ·ç«¯æŒ‚è½½ç›®å½•(æ­£å¸¸è¯»å†™)
```shell
$ ll /mnt
total 147698
-rw-r--r-- 1 root root         4 Oct 10 15:25 aa.txt
...
```
### 3.1.7 osd é»‘åå•çš„å®¢æˆ·ç«¯è¶…æ—¶æ—¶é—´
 - æ—§ç‰ˆæœ¬è¶…æ—¶æ—¶é—´ä¸º 1 å°æ—¶
 - æ–°ç‰ˆæœ¬ 12.2.2 è¶…æ—¶æ—¶é—´ä¸º 300s

**æ€»ç»“ï¼š**
 - å¯ä»¥é€šè¿‡æŒ‡ä»¤ client evict å‰”é™¤æŒ‡å®šçš„å®¢æˆ·ç«¯
 - å‰”é™¤çš„å®¢æˆ·ç«¯ä¼šåŠ å…¥åˆ° osd é»‘åå•ä¸­
 - åŠ å…¥åˆ° osd é»‘åå•ä¸­çš„å®¢æˆ·ç«¯éƒ½ä¸èƒ½è¯»å†™
 - æ¢å¤å‰”é™¤çš„å®¢æˆ·ç«¯éœ€è¦åˆ é™¤ osd é»‘åå•ä¸­çš„å®¢æˆ·ç«¯ä¿¡æ¯
 - åˆ é™¤ osd é»‘åå•ä¸­å®¢æˆ·ç«¯ä¿¡æ¯ï¼Œå®¢æˆ·ç«¯ç«‹é©¬èƒ½æ­£å¸¸è¯»å†™
 - fuse å®¢æˆ·ç«¯å¯ä»¥æ¢å¤ï¼Œkernel å®¢æˆ·ç«¯æ— æ³•æ¢å¤
 - ç»è¿‡è¯•éªŒè¯æ˜ï¼š
    - å‰”é™¤çš„ç”¨æˆ·è™½ç„¶é‡Šæ”¾äº† inode
   - ä¸» mds çš„å†…å­˜å¹¶æœªé‡Šæ”¾
   - ä¸»ä»åˆ‡æ¢åï¼Œå¤‡ mds å†…å­˜ä¼šé‡Šæ”¾
   - ä¸»ä»åˆ‡æ¢åï¼Œåˆ‡æ¢é€Ÿåº¦å°‘äº†åŠ è½½ inode è€—æ—¶çš„é˜¶æ®µï¼Œä»è€ŒåŠ å¿«åˆ‡æ¢é€Ÿåº¦ï¼Œç§’çº§åˆ«

## 3.2 å†…å­˜æœªé‡Šæ”¾åˆ†æ
### 3.2.1 ä¾èµ–è½¯ä»¶
```shell
yum install google-perftools
```
### 3.2.2 æŸ¥çœ‹ mds å†…å­˜
```shell
top - 13:14:06 up 63 days, 21:36,  1 user,  load average: 0.06, 0.08, 0.12
Tasks:   1 total,   0 running,   1 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.2 us,  0.1 sy,  0.0 ni, 99.7 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
KiB Mem : 13149521+total, 96957576 free, 10023744 used, 24513896 buff/cache
KiB Swap:        0 total,        0 free,        0 used. 11539159+avail Mem
  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND
 4997 ceph      20   0 4081012 1.447g  11452 S   0.0  1.2   0:54.29 ceph-mds
```

### 3.2.3 å¯åŠ¨å‰–æå™¨
```shell
$ ceph tell mds.0 heap start_profiler
2018-10-12 13:15:35.979279 7f3430bfa700  0 client.5796596 ms_handle_reset on 192.168.1.2:6804/2252738073
2018-10-12 13:15:36.008686 7f34293fc700  0 client.5796599 ms_handle_reset on 192.168.1.2:6804/2252738073
mds.ceph-xxx-osd01.ys started profiler
```

### 3.2.4 è½¬å‚¨å †æ ˆä¿¡æ¯
```shell
$ ceph tell mds.0 heap dump
2018-10-12 13:16:34.891671 7efd04bfa700  0 client.5796659 ms_handle_reset on 192.168.1.2:6804/2252738073
2018-10-12 13:16:34.922696 7efcfd3fc700  0 client.5796662 ms_handle_reset on 192.168.1.2:6804/2252738073
mds.ceph-xxx-osd01.ys dumping heap profile now.
------------------------------------------------
MALLOC:     1225155304 ( 1168.4 MiB) Bytes in use by application
MALLOC: +            0 (    0.0 MiB) Bytes in page heap freelist
MALLOC: +    289987072 (  276.6 MiB) Bytes in central cache freelist
MALLOC: +     11013456 (   10.5 MiB) Bytes in transfer cache freelist
MALLOC: +      7165384 (    6.8 MiB) Bytes in thread cache freelists
MALLOC: +      7598240 (    7.2 MiB) Bytes in malloc metadata
MALLOC:   ------------
MALLOC: =   1540919456 ( 1469.5 MiB) Actual memory used (physical + swap)
MALLOC: +    112582656 (  107.4 MiB) Bytes released to OS (aka unmapped)
MALLOC:   ------------
MALLOC: =   1653502112 ( 1576.9 MiB) Virtual address space used
MALLOC:
MALLOC:          94545              Spans in use
MALLOC:             16              Thread heaps in use
MALLOC:           8192              Tcmalloc page size
------------------------------------------------
Call ReleaseFreeMemory() to release freelist memory to the OS (via madvise()).
Bytes released to the OS take up virtual address space but no physical memory.
```
### 3.2.5 google-pprof åˆ†æå†…å­˜å †æ ˆ
```shell
pprof --text bin/ceph-mds out/mds.a.profile.0001.heap
$ pprof --text bin/ceph-mds out/mds.a.profile.0008.heap
Using local file bin/ceph-mds.
Using local file out/mds.a.profile.0008.heap.
Total: 46.6 MB
    18.1  38.7%  38.7%     19.5  41.9% Server::prepare_new_inode
     6.2  13.3%  52.0%      6.2  13.3% std::_Rb_tree::_M_emplace_hint_unique (inline)
     5.0  10.7%  62.7%      5.8  12.3% CDir::add_null_dentry
     3.8   8.1%  70.8%      3.8   8.1% std::_Rb_tree::_Rb_tree_impl::_M_initialize (inline)
     3.6   7.7%  78.6%      3.6   7.7% ceph::logging::Log::create_entry
     3.1   6.7%  85.2%      3.1   6.7% Counter::_count (inline)
     2.6   5.5%  90.7%      2.6   5.5% ceph::buffer::raw_combined::create (inline)
     0.9   2.0%  92.8%      0.9   2.0% std::_Vector_base::_M_create_storage (inline)
     0.8   1.6%  94.4%      0.8   1.6% CDir::add_null_dentry (inline)
     0.6   1.2%  95.6%      0.6   1.2% CInode::add_client_cap (inline)
     0.5   1.1%  96.6%      0.5   1.1% std::string::_Rep::_S_create
     0.5   1.0%  97.6%      0.5   1.0% MDCache::add_inode (inline)
     0.2   0.5%  98.2%      0.3   0.6% decode_message
     0.2   0.4%  98.5%      0.2   0.5% MDCache::request_start (inline)
     0.1   0.2%  98.7%      0.1   0.3% CInode::project_inode
     0.1   0.2%  99.0%      0.1   0.2% std::_Rb_tree::_M_insert_unique (inline)
     0.1   0.1%  99.1%      0.1   0.1% std::string::_M_data (inline)
```

# 4. æ€»ç»“

cephfs mds ç›®å‰ç‰ˆæœ¬éƒ½æ˜¯å•æ´»ï¼Œå¯¹äºå¤æ‚å¤šå˜çš„å®¢æˆ·ç«¯å¯èƒ½ä¼šå¸¦æ¥ä¸€å®šçš„æ€§èƒ½å½±å“ã€‚
ä¾‹å¦‚ï¼šåœ¨æœ¬åœ°ç£ç›˜ä¸‹å»æœç´¢ä¸€ä¸ªæ–‡ä»¶ï¼Œå¦‚æœæ–‡ä»¶æ•°è¿‡å¤šçš„ï¼Œå¯¹æœ¬æœº cpu ä»¥åŠè´Ÿè½½ä¹Ÿä¼šå¸¦æ¥ä¸€å®šçš„å†²å‡»ï¼Œæ›´ä½•å†µæ˜¯å¤æ‚å¤šå˜çš„ç½‘ç»œç£ç›˜ã€‚

**ç›®å‰æ¨èçš„ä¼˜åŒ–æ–¹æ¡ˆï¼š**

*   ceph-fuse å®¢æˆ·ç«¯ Qos é™é€Ÿï¼Œé¿å… IO ä¸€ç¬é—´æ¶Œè¿›æ¥å¯¼è‡´ mds æŠ–åŠ¨(ä»å®¢æˆ·ç«¯é™åˆ¶ IOPS,é¿å…èµ„æºäº‰æŠ¢ï¼Œå¯¹ç³»ç»Ÿèµ„æºå¸¦æ¥å†²å‡»)
*   å¤šæ´» mds,Â ç›®å½•åˆ†ç‰‡(æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼Œè¯¦è§ï¼š[å¤šæ´» MDS çš„æ€§èƒ½æµ‹è¯•](https://mp.weixin.qq.com/s/R8TC-S6h_gHPaokTGTqKeQ)Â Â )Â  Â  Â 
*   mds åœ¨ä¸»å¤„ç†æµç¨‹ä¸­ä½¿ç”¨äº†å•çº¿ç¨‹ï¼Œè¿™å¯¼è‡´äº†å…¶å•ä¸ª MDS çš„æ€§èƒ½å—åˆ°äº†é™åˆ¶ï¼Œæœ€å¤§å•ä¸ª MDS å¯è¾¾ 8k ops/sï¼ŒCPU åˆ©ç”¨ç‡è¾¾åˆ°çš„ 140%å·¦å³ã€‚
*   å¦‚æœ mds è´Ÿè½½è¿‡é«˜æˆ–è€…å†…å­˜è¿‡å¤§ï¼Œé™åˆ¶å†…å­˜æˆ–è€…å®šæœŸçš„å›æ”¶ cache(å‡è½» mds çš„å‹åŠ›ï¼Œæå‡ååÂ [https://github.com/ceph/ceph/pull/17711/files](https://github.com/ceph/ceph/pull/17711/files))
*   å‰”é™¤ç”¨æˆ·å¯ä»¥é‡Šæ”¾ inode æ•°é‡ï¼Œä½†æ˜¯ä¸èƒ½å‡å°‘å†…å­˜ï¼Œå¦‚æœæ­¤æ—¶åˆ‡æ¢ä¸»ä»å¯ä»¥åŠ å¿«åˆ‡æ¢é€Ÿåº¦ã€‚
