---
title: ä½¿ç”¨ rook éƒ¨ç½² CEPH
categories: 
  - ğŸ’» å·¥ä½œ
  - å­˜å‚¨
  - CEPH
  - 2-éƒ¨ç½²
date: 2020-12-11 12:49:17
permalink: /pages/05f41e/
tags: 
  - 
---

## ç‰ˆæœ¬ä¿¡æ¯
æ‰€é‡‡ç”¨ç‰ˆæœ¬ä¸º`Rook version v1.2`ï¼›

### Prerequisites

#### å‰æ

> Kubernetes ç‰ˆæœ¬ V1.11+

> ```shell
> modprobe rbd
> ```

> ```shell
> # æ–°ç‰ˆæœ¬ä»¥ceph-volumeä¸ºå­˜å‚¨ä»‹è´¨ç®¡ç†å·¥å…·ï¼ŒåŸºäºlvmå®ç°ç®¡ç†ï¼Œæ›¿æ¢è€çš„å·¥å…·ceph-diskï¼›
> yum install -y lvm2
> ```

> å‚æ•°è°ƒæ•´
>
> ```shell
> # pid max
> sysctl -w kernel.pid_max=4194303
> # é€šè¿‡æ•°æ®é¢„è¯»å¹¶ä¸”è®°è½½åˆ°éšæœºè®¿é—®å†…å­˜æ–¹å¼æé«˜ç£ç›˜è¯»æ“ä½œ
> echo "8192" > /sys/block/${ç›˜ç¬¦}/queue/read_ahead_kb
> # I/O Scheduler
> echo "deadline" > /sys/block/${æ™®é€šç›˜ç¬¦}/queue/scheduler
> echo "noop" > /sys/block/${ssdç›˜ç¬¦}/queue/scheduler
> ```

#### å®‰è£…åŒ…

```shell
git clone --single-branch --branch release-1.2 https://github.com/rook/rook.git
```

#### èŠ‚ç‚¹æ ‡ç­¾åŠæ±¡ç‚¹

```shell
kubectl label nodes $CEPH_NODE role=storage-node
kubectl taint nodes $CEPH_NODE storage-node=:NoSchedule
```

#### å‡†å¤‡é•œåƒ

```shell
# cluster
ceph/ceph:v14.2.9
# operator
rook/ceph:v1.2.7
# csi-ceph
quay.io/cephcsi/cephcsi:v2.0.0
# csi-registrar
quay.io/k8scsi/csi-node-driver-registrar:v1.2.0
# csi-resizer
quay.io/k8scsi/csi-resizer:v0.4.0
# csi-provisioner
quay.io/k8scsi/csi-provisioner:v1.4.0
# csi-snapshotter
quay.io/k8scsi/csi-snapshotter:v1.2.2
# csi-attacher
quay.io/k8scsi/csi-attacher:v2.1.0
```

### Installation

#### ä¸ªæ€§åŒ–é…ç½®

```shell
cd rook/cluster/examples/kubernetes/ceph
vim cluster.yaml
```

```yaml
apiVersion: ceph.rook.io/v1
kind: CephCluster
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  cephVersion:
    image: ceph/ceph:v14.2.9
    allowUnsupported: false
  dataDirHostPath: /var/lib/rook
  skipUpgradeChecks: false
  continueUpgradeAfterChecksEvenIfNotHealthy: false
  mon:
    count: 3
    allowMultiplePerNode: false
  # mgr:
    # modules:
    # Several modules should not need to be included in this list. The "dashboard" and "monitoring" modules
    # are already enabled by other settings in the cluster CR and the "rook" module is always enabled.
    # - name: pg_autoscaler
    #   enabled: true
  # enable the ceph dashboard for viewing cluster status
  dashboard:
    enabled: true
    # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
    # urlPrefix: /ceph-dashboard
    # serve the dashboard at the given port.
    # port: 8443
    # serve the dashboard using SSL
    ssl: true
  # enable prometheus alerting for cluster
  monitoring:
    # requires Prometheus to be pre-installed
    enabled: false
    rulesNamespace: rook-ceph
  network:
    # toggle to use hostNetwork
    hostNetwork: false
  rbdMirroring:
    # The number of daemons that will perform the rbd mirroring.
    # rbd mirroring must be configured with "rbd mirror" from the rook toolbox.
    workers: 0
  # enable the crash collector for ceph daemon crash collection
  crashCollector:
    disable: false
  placement:
    all:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: role
              operator: In
              values:
              - storage-node
      podAffinity:
      podAntiAffinity:
      tolerations:
      - key: storage-node
        operator: Exists
# The above placement information can also be specified for mon, osd, and mgr components
#    mon:
# Monitor deployments may contain an anti-affinity rule for avoiding monitor
# collocation on the same node. This is a required rule when host network is used
# or when AllowMultiplePerNode is false. Otherwise this anti-affinity rule is a
# preferred rule with weight: 50.
#    osd:
#    mgr:
  annotations:
#    all:
#    mon:
#    osd:
# If no mgr annotations are set, prometheus scrape annotations will be set by default.
#   mgr:
  resources:
# The requests and limits set here, allow the mgr pod to use half of one CPU core and 1 gigabyte of memory
# æ­¤å¤„ä¿®æ”¹ç»„ä»¶çš„èµ„æºé…ç½®
    mgr:
      limits:
        cpu: "500m"
        memory: "1024Mi"
      requests:
        cpu: "500m"
        memory: "1024Mi"
# The above example requests/limits can also be added to the mon and osd components
    mon:
# osdå†…å­˜é…ç½®æ¯ä¸ªè‡³å°‘2G
    osd:
#    prepareosd:
#    crashcollector:
  # The option to automatically remove OSDs that are out and are safe to destroy.
  removeOSDsIfOutAndSafeToRemove: false
#  priorityClassNames:
#    all: rook-ceph-default-priority-class
#    mon: rook-ceph-mon-priority-class
#    osd: rook-ceph-osd-priority-class
#    mgr: rook-ceph-mgr-priority-class
  storage: # cluster level storage configuration and selection
    useAllNodes: false
    useAllDevices: false
    #deviceFilter:
    config:
       storeType: bluestore
       metadataDevice: "md0" # ä¿®æ”¹ä¸ºssdç›˜ç¬¦
      # databaseSizeMB: "1024" # uncomment if the disks are smaller than 100 GB
      # journalSizeMB: "1024"  # uncomment if the disks are 20 GB or smaller
       osdsPerDevice: "1" # this value can be overridden at the node or device level
      # encryptedDevice: "true" # the default value for this option is "false"
# Cluster level list of directories to use for filestore-based OSD storage. If uncomment, this example would create an OSD under the dataDirHostPath.
    #directories:
    #- path: /var/lib/rook
# æ­¤å¤„ä¿®æ”¹å­˜å‚¨èŠ‚ç‚¹ç›¸å…³ä¿¡æ¯åŠç£ç›˜é…ç½®
    nodes:
    - name: "172.17.4.201"
      devices: 
      - name: "sdb" # è£¸è®¾å¤‡ç›˜ç¬¦,å¤šä¸ªè®¾å¤‡ç½—åˆ—
      - name: "nvme01" # æ€§èƒ½è¾ƒå¥½çš„ç£ç›˜,å•ä¸ªè®¾å¤‡éœ€è¦å¤šä¸ªosd
        config:
          osdsPerDevice: "5"
      config: # æ­¤å¤„é…ç½®ä¼šè¦†ç›–ä¸Šé¢çš„å…¨å±€é…ç½®,éœ€ä¸ä¸Šä¿æŒä¸€è‡´,æˆ–é’ˆå¯¹èŠ‚ç‚¹å®šåˆ¶
        storeType: bluestore
        metadataDevice: "md0" # ä¿®æ”¹ä¸ºssdç›˜ç¬¦
#    - name: "172.17.4.301"
#      deviceFilter: "^sd."
  disruptionManagement:
    managePodBudgets: false
    osdMaintenanceTimeout: 30
    manageMachineDisruptionBudgets: false
    machineDisruptionBudgetNamespace: openshift-machine-api
```

> æ³¨æ„ï¼šèµ„æºé…ç½®å‚ç…§https://docs.ceph.com/docs/master/start/hardware-recommendations/

#### æ‰§è¡Œå®‰è£…

```shell
cd rook/cluster/examples/kubernetes/ceph
# create the common resources
kubectl create -f common.yaml
# create the Operator deployment
kubectl create -f operator.yaml
# create Ceph storage cluster
kubectl create -f cluster.yaml
```

#### åˆ›å»º toolbox

```shell
kubectl create -f toolbox.yaml
```

#### åˆ›å»º dashboard

```shell
# æŒ‰éœ€æ±‚é€‰æ‹©
kubectl create -f [dashboard-external-http.yaml | dashboard-external-https.yaml | dashboard-ingress-https.yaml | dashboard-loadbalancer.yaml]
```

#### åˆ›å»ºå—å­˜å‚¨

*Kubernetes ä½¿ç”¨*

```shell
# æ ¹æ®éœ€æ±‚ä¿®æ”¹pool nameã€RBD image formatã€RBD image featuresç­‰
kubectl create -f csi/rbd/storageclass.yaml
# åˆ›å»ºpvc
kubectl create -f csi/rbd/pvc.yaml
```

*é Kubernetes ä½¿ç”¨*

```shell
# æ ¹æ®éœ€æ±‚ä¿®æ”¹pool nameï¼Œpgã€pgpå¯åç»­é€šè¿‡toolboxæ‰‹åŠ¨è°ƒæ•´
kubectl create -f pool.yaml
# imgé€šè¿‡toolboxæ‰‹åŠ¨åˆ›å»º
```

#### åˆ›å»ºæ–‡ä»¶ç³»ç»Ÿ

*Kubernetes ä½¿ç”¨*

```shell
# æ ¹æ®éœ€æ±‚ä¿®æ”¹fs nameã€è°ƒåº¦äº²å’ŒåŠmdsèµ„æºåˆ†é…
kubectl create -f filesystem.yaml
kubectl create -f csi/cephfs/storageclass.yaml
kubectl create -f csi/cephfs/pvc.yaml
```

*é Kubernetes ä½¿ç”¨*

```shell
# æ ¹æ®éœ€æ±‚ä¿®æ”¹fs nameã€è°ƒåº¦äº²å’ŒåŠmdsèµ„æºåˆ†é…
kubectl create -f filesystem.yaml
```

### Cleaning up a Cluster

*Delete the Block and File artifacts*

```shell
kubectl delete -f ../wordpress.yaml
kubectl delete -f ../mysql.yaml
kubectl delete -n rook-ceph cephblockpool replicapool
kubectl delete storageclass rook-ceph-block
kubectl delete -f csi/cephfs/kube-registry.yaml
kubectl delete storageclass csi-cephfs
```

*Delete the CephCluster CRD*

```shell
kubectl -n rook-ceph delete cephcluster rook-ceph
```

*Delete the Operator and related Resources*

```shell
kubectl delete -f operator.yaml
kubectl delete -f common.yaml
```

*Delete the data on hosts*

```shell
#!/usr/bin/env bash
DISK="/dev/sdb"
# Zap the disk to a fresh, usable state (zap-all is important, b/c MBR has to be clean)
# You will have to run this step for all disks.
sgdisk --zap-all $DISK

# These steps only have to be run once on each node
# If rook sets up osds using ceph-volume, teardown leaves some devices mapped that lock the disks.
ls /dev/mapper/ceph-* | xargs -I% -- dmsetup remove %
# ceph-volume setup can leave ceph-<UUID> directories in /dev (unnecessary clutter)
rm -rf /dev/ceph-*
```

*clean up `dataDirHostPath`*

```shell
rm -rf /var/lib/rook
```
## å‚è€ƒèµ„æ–™

[å®˜æ–¹æ–‡æ¡£](https://rook.github.io/docs/rook/v1.2/)