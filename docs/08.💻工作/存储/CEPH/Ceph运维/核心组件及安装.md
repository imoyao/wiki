---
title: æ ¸å¿ƒç»„ä»¶åŠå®‰è£…

tags: ceph
categories: 
  - ğŸ’» å·¥ä½œ
  - å­˜å‚¨
  - CEPH
  - Ceph è¿ç»´
date: 2020-05-23 11:02:28
permalink: /pages/ef6879/
---

### æ¦‚è§ˆ

- `Ceph`çš„åº•å±‚æ ¸å¿ƒæ˜¯`RADOSï¼ˆReliable, Autonomic Distributed Object Storeï¼‰`,å…¶æœ¬è´¨æ˜¯ä¸€ä¸ªå¯¹è±¡å­˜å‚¨ï¼›
- `RADOS`ç”±ä¸¤ä¸ªç»„ä»¶ç»„æˆï¼š`OSD`å’Œ`Monitor`ï¼›`OSD`ä¸»è¦æä¾›å­˜å‚¨èµ„æºï¼Œæ¯ä¸€ä¸ª`disk`ã€`SSD`ã€`RAID group`æˆ–è€…ä¸€ä¸ªåˆ†åŒºéƒ½å¯ä»¥æˆä¸ºä¸€ä¸ª`OSD`ï¼Œè€Œæ¯ä¸ª`OSD`è¿˜å°†è´Ÿè´£å‘è¯¥å¯¹è±¡çš„å¤æ‚èŠ‚ç‚¹åˆ†å‘å’Œæ¢å¤ï¼›`Monitor`ç»´æŠ¤`Ceph`é›†ç¾¤å¹¶ç›‘æ§`Ceph`é›†ç¾¤çš„å…¨å±€çŠ¶æ€ï¼Œæä¾›ä¸€è‡´æ€§çš„å†³ç­–ï¼›
- `RADOS`åˆ†å‘ç­–ç•¥ä¾èµ–äºåä¸º`CRUSHï¼ˆControlled Replication Under Scalable Hashingï¼‰`çš„ç®—æ³•ï¼ˆåŸºäºå¯æ‰©å±•å“ˆå¸Œç®—æ³•çš„å¯æ§å¤åˆ¶ï¼‰ï¼›
- é€šè¿‡ CRUSH ç®—æ³•åŠ¨æ€è®¡ç®—å…ƒæ•°æ®ï¼Œå»ä¸­å¿ƒåŒ–ï¼Œä¸éœ€è¦ç»Ÿä¸€ç®¡ç†ä¸€ä¸ªé›†ä¸­å¼çš„å…ƒæ•°æ®è¡¨ï¼›
- CRUSH è¿˜æœ‰ä¸€ä¸ªç‹¬ç‰¹çš„åŸºç¡€è®¾æ–½æ„ŸçŸ¥èƒ½åŠ›ï¼Œå®ƒèƒ½äº†è§£åŸºç¡€è®¾æ–½ä¸­ä¸åŒç»„ä»¶ä¹‹é—´çš„å…³ç³»ï¼Œä»æœ€åˆçš„ç³»ç»Ÿç£ç›˜ã€æ± ã€èŠ‚ç‚¹ã€æœºæ¶ã€ç”µæ˜æ’æ¿ã€äº¤æ¢æœºåˆ°ç°åœ¨çš„æ•°æ®ä¸­å¿ƒï¼Œä»¥åŠæ•°æ®ä¸­å¿ƒæˆ¿é—´ç­‰ è¿™äº›éƒ½æ˜¯ä»»ä½•åŸºç¡€è®¾æ–½ä¸­çš„æ•…éšœåŒºåŸŸï¼ŒCRUSH ä¼šä»¥å¤šå‰¯æœ¬çš„æ–¹å¼ä¿å­˜æ•°æ®ï¼Œä»¥ä¿è¯åœ¨æ•…éšœåŒºåŸŸä¸­æœ‰äº›ç»„ä»¶æ•…éšœçš„æƒ…å†µä¸‹æ•°æ®ä¾æ—§å¯ç”¨ï¼›
- ä¸€ä¸ªå¯¹è±¡é€šå¸¸åŒ…å«ç»‘å®šåœ¨ä¸€èµ·çš„æ•°æ®å’Œå…ƒæ•°æ®ï¼Œå¹¶ä¸”ç”¨ä¸€ä¸ªå…¨å±€å”¯ä¸€çš„æ ‡è¯†ç¬¦ï¼›
- CRUSH æŸ¥æ‰¾ï¼šå…ƒæ•°æ®è®¡ç®—çš„è´Ÿè½½æ˜¯åˆ†å¸ƒå¼çš„å¹¶ä¸”åªåœ¨éœ€è¦æ—¶æ‰§è¡Œï¼Œå…ƒæ•°æ®è®¡ç®—è¿‡ç¨‹ä¹Ÿç§°ä¸º CRUSH æŸ¥æ‰¾ï¼›å®¢æˆ·ç«¯ä½¿ç”¨è‡ªå·±çš„ç³»ç»Ÿèµ„æºæ¥æ‰§è¡Œ CRUSH æŸ¥æ‰¾ã€‚ä»è€Œå–æ¶ˆä¸­å¿ƒæŸ¥æ‰¾ï¼›
- å¯¹äº Ceph é›†ç¾¤çš„ä¸€æ¬¡è¯»å†™æ“ä½œï¼Œå®¢æˆ·ç«¯é¦–å…ˆè¯·æ±‚ Ceph çš„ monitor å¹¶è·å–ä¸€ä¸ª cluster map å‰¯æœ¬ï¼›cluster map å¸®åŠ©å®¢æˆ·ç«¯è·å– Ceph é›†ç¾¤çš„çŠ¶æ€å’Œé…ç½®ä¿¡æ¯ï¼›ä½¿ç”¨å¯¹è±¡å’Œæ± å/ID å°†æ•°æ®è½¬æ¢ä¸ºå¯¹è±¡ï¼›ç„¶åå°†å¯¹è±¡å’Œ PG(Placement Groups)æ•°ä¸€èµ·ç»è¿‡æ•£åˆ—æ¥ç”Ÿæˆå…¶åœ¨ Ceph æ± ä¸­æœ€ç»ˆå­˜æ”¾çš„ä¸€ä¸ª PGï¼›ç„¶åå‰é¢è®¡ç®—å¥½çš„ PG ç»è¿‡ CRUSH æŸ¥æ‰¾æ¥ç¡®å®šå­˜å‚¨æˆ–è·å–æ•°æ®æ‰€éœ€çš„ä¸» OSD ä½ç½®ï¼›è®¡ç®—å®Œå‡†ç¡®çš„ OSD ID ä¹‹åï¼Œå®¢æˆ·ç«¯ç›´æ¥è”ç³»è¿™ä¸ª OSD æ¥å­˜å‚¨æ•°æ®ï¼›æ‰€æœ‰è¿™äº›è®¡ç®—æ“ä½œéƒ½æ˜¯ç”±å®¢æˆ·ç«¯æ¥æ‰§è¡Œï¼Œå› æ­¤å®ƒä¸ä¼šå½±å“é›†ç¾¤çš„æ€§èƒ½ï¼›ä¸€æ—¦æ•°æ®å†™å…¥ä¸» OSDï¼Œä¸» OSD æ‰€åœ¨èŠ‚ç‚¹å°†æ‰§è¡Œ CRUSH æŸ¥æ‰¾æ“ä½œå¹¶è®¡ç®—è¾…åŠ©å½’ç½®ç»„å’Œ OSD çš„ä½ç½®æ¥å®ç°æ•°æ®å¤åˆ¶ï¼Œè¿›è€Œå®ç°é«˜å¯ç”¨æ€§ï¼›

### Ceph çš„æ ¸å¿ƒç»„ä»¶

- [ ] Monitorï¼šä¸€ä¸ª Ceph é›†ç¾¤éœ€è¦å¤šä¸ª Monitor ç»„æˆçš„å°é›†ç¾¤ï¼Œå®ƒä»¬é€šè¿‡ Paxos åŒæ­¥æ•°æ®ï¼Œç”¨æ¥ä¿å­˜ OSD çš„å…ƒæ•°æ®ï¼›Ceph monitor(MON)ç»„ä»¶é€šè¿‡ä¸€ç³»åˆ—çš„ map æ¥è·Ÿè¸ªæ•´ä¸ªé›†ç¾¤çš„å¥åº·çŠ¶æ€ï¼ŒåŒ…æ‹¬ OSDã€MONã€PG å’Œ CRUSH ç­‰ç»„ä»¶çš„ mapï¼›æ‰€æœ‰çš„é›†ç¾¤èŠ‚ç‚¹éƒ½å‘ monitor èŠ‚ç‚¹æŠ¥å‘ŠçŠ¶æ€ï¼Œå¹¶åˆ†äº«æ¯ä¸€ä¸ªçŠ¶æ€å˜åŒ–çš„ä¿¡æ¯ã€‚ä¸€ä¸ª monitor ä¸ºæ¯ä¸€ä¸ªç»„ä»¶ç»´æŠ¤ä¸€ä¸ªç‹¬ç«‹çš„ mapï¼›monitor ä¸å­˜å‚¨å®é™…æ•°æ®ï¼›
- [ ] OSDï¼šObject Storage Deviceï¼Œè´Ÿè´£å°†å®é™…çš„æ•°æ®ä»¥å¯¹è±¡çš„å½¢å¼å­˜å‚¨åœ¨æ¯ä¸€ä¸ªé›†ç¾¤èŠ‚ç‚¹çš„ç‰©ç†ç£ç›˜é©±åŠ¨å™¨ä¸­ï¼Œä¸€ä¸ª Ceph é›†ç¾¤ä¸€èˆ¬æœ‰å¤šä¸ª OSDï¼›
- [ ] MDSï¼šCeph Metadata Serverï¼Œæ˜¯ CephFS æœåŠ¡ä¾èµ–çš„å…ƒæ•°æ®æœåŠ¡ï¼›
- [ ] Objectï¼šCeph æœ€åº•å±‚çš„å­˜å‚¨å•å…ƒæ˜¯ Objectï¼Œå¯¹è±¡åŒ…å«ä¸€ä¸ªæ ‡è¯†ç¬¦ã€äºŒè¿›åˆ¶æ•°æ®ã€å’Œç”±åå­—/å€¼å¯¹ç»„æˆçš„å…ƒæ•°æ®ï¼Œå…ƒæ•°æ®è¯­ä¹‰å®Œå…¨å–å†³äº Ceph å®¢æˆ·ç«¯ï¼›
- [ ] PGï¼šå…¨ç§° Placement Groupsï¼Œæ˜¯ä¸€ä¸ªé€»è¾‘æ¦‚å¿µï¼Œä¸€ä¸ª PG åŒ…å«å¤šä¸ª OSDï¼Œå¼•å…¥è¿™ä¸€å±‚æ˜¯ä¸ºäº†æ›´å¥½åœ°åˆ†é…æ•°æ®å’Œå®šä½æ•°æ®ï¼›
- [ ] RADOSï¼šå…¨ç§° Reliable Autonomic Distribute Object Storeï¼Œæ˜¯ Ceph é›†ç¾¤çš„ç²¾åï¼Œç”¨æˆ·å®ç°æ•°æ®åˆ†é…ã€Failover ç­‰é›†ç¾¤æ“ä½œï¼›
- [ ] Libradosï¼šæ˜¯ Rados æä¾›åº“ï¼Œå› ä¸º RADOS æ˜¯åè®®å¾ˆéš¾ç›´æ¥è®¿é—®ï¼Œå› æ­¤ä¸Šå±‚çš„ RBDã€RGW å’Œ CephFS éƒ½æ˜¯é€šè¿‡ Librados è®¿é—®ï¼›
- [ ] CRUSHï¼šæ˜¯ Ceph ä½¿ç”¨çš„æ•°æ®åˆ†å¸ƒç®—æ³•ï¼Œç±»ä¼¼ä¸€è‡´æ€§å“ˆå¸Œï¼Œè®©æ•°æ®åˆ†é…åˆ°é¢„æœŸçš„åœ°æ–¹ï¼›
- [ ] RBDï¼šå…¨ç§° RADOS Block Deviceï¼Œæ˜¯ Ceph å¯¹å¤–æä¾›çš„å—è®¾å¤‡æœåŠ¡ï¼›
- [ ] RGWï¼šå…¨ç§° RADOS Gatewayï¼Œæ˜¯ Ceph å¯¹å¤–æä¾›çš„å¯¹è±¡å­˜å‚¨æœåŠ¡ï¼Œæ¥å£ä¸ S3 å’Œ Swift å…¼å®¹ï¼›
- [ ] CephFSï¼šå…¨ç§° Ceph File Systemï¼Œæä¾›äº†ä¸€ä¸ªä»»æ„å¤§å°ä¸”å…¼å®¹ POSIX çš„åˆ†å¸ƒå¼æ–‡ä»¶ç³»ç»Ÿï¼›

### ç¡¬ä»¶éœ€æ±‚

```shell
Processorï¼š
Ceph å…ƒæ•°æ®æœåŠ¡å™¨å¯¹ CPU æ•æ„Ÿï¼Œå®ƒä¼šåŠ¨æ€åœ°é‡åˆ†å¸ƒå®ƒä»¬çš„è´Ÿè½½ï¼Œæ‰€ä»¥ä½ çš„å…ƒæ•°æ®æœåŠ¡å™¨åº”è¯¥æœ‰è¶³å¤Ÿçš„å¤„ç†èƒ½åŠ›ï¼ˆå¦‚ 4 æ ¸æˆ–æ›´å¼ºæ‚çš„ CPU ï¼‰ã€‚ Ceph çš„ OSD è¿è¡Œç€ RADOS æœåŠ¡ã€ç”¨ CRUSH è®¡ç®—æ•°æ®å­˜æ”¾ä½ç½®ã€å¤åˆ¶æ•°æ®ã€ç»´æŠ¤å®ƒè‡ªå·±çš„é›†ç¾¤è¿è¡Œå›¾å‰¯æœ¬ï¼Œå› æ­¤ OSD éœ€è¦ä¸€å®šçš„å¤„ç†èƒ½åŠ›ï¼ˆå¦‚åŒæ ¸ CPU ï¼‰ã€‚ç›‘è§†å™¨åªç®€å•åœ°ç»´æŠ¤ç€é›†ç¾¤è¿è¡Œå›¾çš„å‰¯æœ¬ï¼Œå› æ­¤å¯¹ CPU ä¸æ•æ„Ÿï¼›ä½†å¿…é¡»è€ƒè™‘æœºå™¨ä»¥åæ˜¯å¦è¿˜ä¼šè¿è¡Œ Ceph ç›‘è§†å™¨ä»¥å¤–çš„ CPU å¯†é›†å‹ä»»åŠ¡ã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœåŠ¡å™¨ä»¥åè¦è¿è¡Œç”¨äºè®¡ç®—çš„è™šæ‹Ÿæœºï¼ˆå¦‚ OpenStack Nova ï¼‰ï¼Œä½ å°±è¦ç¡®ä¿ç»™ Ceph è¿›ç¨‹ä¿ç•™äº†è¶³å¤Ÿçš„å¤„ç†èƒ½åŠ›ï¼Œæ‰€ä»¥æˆ‘ä»¬æ¨èåœ¨å…¶ä»–æœºå™¨ä¸Šè¿è¡Œ CPU å¯†é›†å‹ä»»åŠ¡ã€‚
Memoryï¼š
å…ƒæ•°æ®æœåŠ¡å™¨å’Œç›‘è§†å™¨å¿…é¡»å¯ä»¥å°½å¿«åœ°æä¾›å®ƒä»¬çš„æ•°æ®ï¼Œæ‰€ä»¥ä»–ä»¬åº”è¯¥æœ‰è¶³å¤Ÿçš„å†…å­˜ï¼Œè‡³å°‘æ¯è¿›ç¨‹ 1GB ã€‚ OSD çš„æ—¥å¸¸è¿è¡Œä¸éœ€è¦é‚£ä¹ˆå¤šå†…å­˜ï¼ˆå¦‚æ¯è¿›ç¨‹ 500MB ï¼‰å·®ä¸å¤šäº†ï¼›ç„¶è€Œåœ¨æ¢å¤æœŸé—´å®ƒä»¬å ç”¨å†…å­˜æ¯”è¾ƒå¤§ï¼ˆå¦‚æ¯è¿›ç¨‹æ¯ TB æ•°æ®éœ€è¦çº¦ 1GB å†…å­˜ï¼‰ã€‚é€šå¸¸å†…å­˜è¶Šå¤šè¶Šå¥½ã€‚
ç½‘ç»œï¼š
ä½¿ç”¨ä¸‡å…†ç½‘ç»œï¼Œä¸”åˆ†ç¦»clientå’Œclusterçš„ç½‘ç»œï¼›
å­˜å‚¨ï¼š
OSD Journalä½¿ç”¨ssdï¼ˆå› ä¸º Ceph å‘é€ ACK å‰å¿…é¡»æŠŠæ‰€æœ‰æ•°æ®å†™å…¥æ—¥å¿—ï¼ˆè‡³å°‘å¯¹ xfs å’Œ ext4 æ¥è¯´æ˜¯ï¼‰ï¼Œå› æ­¤å‡è¡¡æ—¥å¿—å’Œ OSD æ€§èƒ½ç›¸å½“é‡è¦ï¼‰ï¼›
```

- [ ] CPUï¼šmds >= 4 Coresï¼›osd >= 2 Coresï¼›mon >= 1coresï¼›
- [ ] MEMï¼šmds >= 1GBï¼›osd >= 500MB(é¡»å°†æ•°æ®æ¢å¤æ—¶ä½¿ç”¨çš„èµ„æºè®¡ç®—åœ¨å†…ï¼æ¯è¿›ç¨‹æ¯ TB æ•°æ®éœ€è¦çº¦ 1GB å†…å­˜)
- [ ] å•ä¸ªé©±åŠ¨å™¨å®¹é‡è¶Šå¤§ï¼Œå…¶å¯¹åº”çš„ osd æ‰€éœ€å†…å­˜å°±è¶Šå¤§ï¼Œç‰¹åˆ«æ˜¯åœ¨é‡æƒè¡¡ã€å›å¡«ã€æ¢å¤æœŸé—´ï¼Œæ ¹æ®ç»éªŒï¼Œ1TB çš„å­˜å‚¨ç©ºé—´å¤§çº¦éœ€è¦ 1GB å†…å­˜ï¼›
- [ ] client ä¸ cluster ç½‘ç»œéš”ç¦»ï¼›
- [ ] æ¯ä¸ª osd ç‹¬å ä¸€ä¸ªç£ç›˜é©±åŠ¨å™¨ï¼›
- [ ] å®˜æ–¹ä¸æ¨èä½¿ç”¨ raidï¼Œä¼šé™ä½æ€§èƒ½ï¼Œè‹¥åšæŒä½¿ç”¨ï¼Œæ¨è raid0 å’Œ jbodï¼›
- [ ] è¿è¡Œ ceph çš„èŠ‚ç‚¹ä¸è¦è¿è¡Œå…¶ä»–è¿›ç¨‹ï¼Œä¸”åœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸Šåªè¿è¡Œä¸€ç§ç±»å‹çš„å®ˆæŠ¤è¿›ç¨‹ï¼›
- [ ] ç³»ç»Ÿã€æ•°æ®å’Œæ—¥å¿—å­˜å‚¨åœ¨ä¸åŒçš„ç£ç›˜;

### ç›´æ¥å®‰è£…

```shell
é…ç½®è½¯ä»¶æºï¼Œæ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œï¼›
# cat >/etc/yum.repos.d/ceph.repo<<EOF
[ceph]
name=ceph
baseurl=https://mirrors.aliyun.com/ceph/rpm-hammer/el7/x86_64/
gpgcheck=0
priority=1

[ceph-noarch]
name=cephnoarch
baseurl=https://mirrors.aliyun.com/ceph/rpm-hammer/el7/noarch/
gpgcheck=0
priority=1

[ceph-source]
name=Ceph source packages
baseurl=https://mirrors.aliyun.com/ceph/rpm-hammer/el7/SRPMS
enabled=0
gpgcheck=1
type=rpm-md
gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc
priority=1
EOF

# yum makecache

# yum install ntp ntpdate ntp-doc

æ·»åŠ è¿è¡Œè¿›ç¨‹çš„ç”¨æˆ·ï¼Œæ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œï¼›
# useradd ceph
# echo 'ceph' | passwd --stdin ceph
# echo "ceph ALL = (root) NOPASSWD:ALL" > /etc/sudoers.d/ceph
# chmod 0440 /etc/sudoers.d/ceph
# é…ç½®sshdå¯ä»¥ä½¿ç”¨passwordç™»å½•
# sed -i 's/PasswordAuthentication no/PasswordAuthentication yes/' /etc/ssh/sshd_config
# systemctl reload sshd
# é…ç½®sudoä¸éœ€è¦tty
# sed -i 's/Default requiretty/#Default requiretty/' /etc/sudoers

å°†æ‰€æœ‰èŠ‚ç‚¹æ·»åŠ è‡³hostsæ–‡ä»¶ï¼›
# cat >>/etc/hosts<<EOF
10.205.0.138  yfb-0-138
10.205.0.139  yfb-0-139
10.205.0.140  yfb-0-140
# æ­¤å¤„hostsé…ç½®ä¸€å®šè¦ä¸ä¸»æœºåä¿æŒä¸€è‡´ï¼ï¼ï¼
EOF

å®‰è£…ç¨‹åºåŒ…ï¼Œæ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œï¼›
# yum install -y ceph ceph-radosgw # ç›¸å½“äºæ‰§è¡Œäº†ceph-deploy install

æ·»åŠ é˜²ç«å¢™è§„åˆ™ï¼Œæ‰€æœ‰èŠ‚ç‚¹æ‰§è¡Œï¼›
# iptables -A INPUT -i br0 -p tcp -s 10.205.0.0/16 --dport 6789 -j ACCEPT


éƒ¨ç½²é›†ç¾¤ï¼ŒadminèŠ‚ç‚¹æ‰§è¡Œï¼›
# yum install ceph-deploy -y
# su - ceph
# ssh-keygen -t rsa -P ''

# ssh-copy-id ceph@yfb-0-138
# ssh-copy-id ceph@yfb-0-139
# ssh-copy-id ceph@yfb-0-140

# mkdir ceph-cluster
# cd ceph-cluster
# ceph-deploy new ceph1
éƒ¨ç½²monitorï¼Œç”Ÿæˆkeyï¼›
# ceph-deploy --overwrite-conf mon create-initial
åˆ†å‘keyåˆ°å…¶ä»–èŠ‚ç‚¹ï¼›
# ceph-deploy admin yfb-0-138 yfb-0-139 yfb-0-140

åœ¨èŠ‚ç‚¹åˆ›å»ºç›®å½•ï¼›
# mkdir -pv /data/osd1
# chmod 777 -R /data/osd1
# chown ceph:ceph -R /data/osd1

æ·»åŠ osdï¼›
# ceph-deploy osd prepare yfb-0-138:/data/osd1 yfb-0-139:/data/osd1 yfb-0-140:/data/osd1
# ceph-deploy osd activate yfb-0-138:/data/osd1 yfb-0-139:/data/osd1 yfb-0-140:/data/osd1

åˆ›å»ºå…ƒæ•°æ®æœåŠ¡ï¼›
# ceph-deploy mds create yfb-0-140
åˆ›å»ºRGWï¼›
# ceph-deploy rgw create yfb-0-139
RGW ä¾‹ç¨‹é»˜è®¤ä¼šç›‘å¬ 7480 ç«¯å£ï¼Œå¯ä»¥æ›´æ”¹è¯¥èŠ‚ç‚¹ ceph.conf å†…ä¸ RGW ç›¸å…³çš„é…ç½®ï¼Œå¦‚ä¸‹ï¼š
[client]
rgw frontends = civetweb port=80
```

###  è§£å†³å®‰è£…è¿‡ç¨‹ä¸­é‡åˆ°çš„é—®é¢˜

```shell
1ã€--> è§£å†³ä¾èµ–å…³ç³»å®Œæˆ
é”™è¯¯ï¼šè½¯ä»¶åŒ…ï¼š1:librbd1-0.94.10-0.el7.x86_64 (ceph)
          éœ€è¦ï¼šliblttng-ust.so.0()(64bit)
é”™è¯¯ï¼šè½¯ä»¶åŒ…ï¼š1:ceph-radosgw-0.94.10-0.el7.x86_64 (ceph)
          éœ€è¦ï¼šlibfcgi.so.0()(64bit)
é”™è¯¯ï¼šè½¯ä»¶åŒ…ï¼š1:ceph-0.94.10-0.el7.x86_64 (ceph)
          éœ€è¦ï¼šliblttng-ust.so.0()(64bit)
é”™è¯¯ï¼šè½¯ä»¶åŒ…ï¼š1:ceph-common-0.94.10-0.el7.x86_64 (ceph)
          éœ€è¦ï¼šlibbabeltrace.so.1()(64bit)
é”™è¯¯ï¼šè½¯ä»¶åŒ…ï¼š1:librados2-0.94.10-0.el7.x86_64 (ceph)
          éœ€è¦ï¼šliblttng-ust.so.0()(64bit)
é”™è¯¯ï¼šè½¯ä»¶åŒ…ï¼š1:ceph-common-0.94.10-0.el7.x86_64 (ceph)
          éœ€è¦ï¼šlibbabeltrace-ctf.so.1()(64bit)
é”™è¯¯ï¼šè½¯ä»¶åŒ…ï¼š1:ceph-0.94.10-0.el7.x86_64 (ceph)
          éœ€è¦ï¼šlibleveldb.so.1()(64bit)
 æ‚¨å¯ä»¥å°è¯•æ·»åŠ  --skip-broken é€‰é¡¹æ¥è§£å†³è¯¥é—®é¢˜
 æ‚¨å¯ä»¥å°è¯•æ‰§è¡Œï¼šrpm -Va --nofiles --nodigest

 
# yum install -y yum-utils && yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ && yum install --nogpgcheck -y epel-release && rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 && rm /etc/yum.repos.d/dl.fedoraproject.org*
 

2ã€[ceph@yfb-0-140 ~]$ ceph health 
2019-01-17 15:01:59.790209 7f16e2c07700 -1 monclient(hunting): ERROR: missing keyring, cannot use cephx for authentication
2019-01-17 15:01:59.790219 7f16e2c07700  0 librados: client.admin initialization error (2) No such file or directory
Error connecting to cluster: ObjectNotFound
[ceph@yfb-0-140 ~]$ sudo chmod 755 /etc/ceph/ceph.client.admin.keyring

3ã€# ceph-deploy mon add yfb-0-140
[yfb-0-140][INFO  ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.yfb-0-140.asok mon_status
[yfb-0-140][ERROR ] admin_socket: exception getting command descriptions: [Errno 2] No such file or directory

# cd ceph-cluster
# vim ceph.conf
public_network = 10.205.0.140/16
# ceph-deploy --overwrite-conf config push yfb-0-138 yfb-0-139 yfb-0-140

4ã€[ceph@yfb-0-140 ~]$ ceph health 
HEALTH_WARN 1 pgs degraded; 1 pgs stuck degraded; 1 pgs stuck unclean; 1 pgs stuck undersized; 1 pgs undersized; recovery 14/129 objects degraded (10.853%)

osdè¿›ç¨‹æ— å¼‚å¸¸ï¼Œç£ç›˜ç©ºé—´ä¸è¶³ï¼Œä¸€å®šè¦ç¡®ä¿ç£ç›˜ç©ºé—´å……è¶³ï¼›

5ã€osd init failed: (36) File name too long
Cephå®˜ç½‘å»ºè®®ä½¿ç”¨XFSä½œä¸ºOSDå­˜å‚¨æ•°æ®çš„æ–‡ä»¶ç³»ç»Ÿï¼Œä½†ä½¿ç”¨çš„æ–‡ä»¶ç³»ç»Ÿæ˜¯ext4ï¼Œè€Œext4å­˜å‚¨xattrsçš„å¤§å°æœ‰é™åˆ¶ï¼Œä½¿å¾—OSDä¿¡æ¯ä¸èƒ½å®‰å…¨çš„ä¿å­˜ï¼›å› æ­¤å°±æœ‰ä¸¤ç§æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼š 
	aã€ä¿®æ”¹Cephé…ç½®æ–‡ä»¶çš„osdé€‰é¡¹ã€‚å°†ä¸‹é¢çš„ä¿¡æ¯æ·»åŠ åˆ°Cephé…ç½®æ–‡ä»¶ä¸­globalçš„sectionä¸­ï¼ŒCephé›†ç¾¤ä¸­ï¼Œå¦‚æœosdå­˜å‚¨æ•°æ®çš„æ–‡ä»¶ç³»ç»Ÿæ˜¯ext4çš„ï¼Œéƒ½éœ€è¦ä¿®æ”¹è¿™ä¸ªé…ç½®æ–‡ä»¶ï¼›ç„¶åé‡å¯å¯¹åº”çš„osdæœåŠ¡ï¼›

osd max object name len = 256 
osd max object namespace len = 64 

	bã€ å°†æ–‡ä»¶ç³»ç»Ÿæ”¹ä¸ºXFSï¼›

6ã€]# docker exec mon rbd map test --pool test
rbd: sysfs write failed
RBD image feature set mismatch. Try disabling features unsupported by the kernel with "rbd feature disable".
In some cases useful info is found in syslog - try "dmesg | tail".
rbd: map failed: (6) No such device or address
ç¦ç”¨å½“å‰ç³»ç»Ÿå†…æ ¸ä¸æ”¯æŒçš„featureï¼š
]# docker exec mon rbd info test/test
rbd image 'test':
	size 1 GiB in 256 objects
	order 22 (4 MiB objects)
	id: 18166b8b4567
	block_name_prefix: rbd_data.18166b8b4567
	format: 2
	features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
	op_features: 
	flags: 
	create_timestamp: Fri Aug 23 13:16:06 2019
]# docker exec mon rbd feature disable test exclusive-lock, object-map, fast-diff, deep-flatten --pool test

rbd infoæ˜¾ç¤ºçš„RBDé•œåƒçš„formatä¸º2ï¼ŒFormat 2çš„RBDé•œåƒæ”¯æŒRBDåˆ†å±‚ï¼Œæ˜¯å®ç°Copy-On-Writeçš„å‰ææ¡ä»¶

```